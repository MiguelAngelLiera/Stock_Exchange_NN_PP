{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from NARNN import NARNN\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(type(criterion))\n",
    "red_A1 = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de la capa: capa1\n",
      "Pesos:\n",
      "Parameter containing:\n",
      "tensor([[ 0.3144, -0.0202, -0.0108, -0.0429, -0.1464, -0.2823,  0.0104,  0.2925,\n",
      "          0.2730,  0.2427],\n",
      "        [ 0.1854,  0.3144,  0.0367, -0.2001, -0.0542, -0.1495,  0.0650, -0.0174,\n",
      "          0.1961, -0.0806],\n",
      "        [-0.0571, -0.1487, -0.1379,  0.1222,  0.0623, -0.1982,  0.3077, -0.1953,\n",
      "          0.2836, -0.1059],\n",
      "        [ 0.2721,  0.0785,  0.2622,  0.2319, -0.2343, -0.1718,  0.2329, -0.0472,\n",
      "          0.0037, -0.1467],\n",
      "        [ 0.1288,  0.2063, -0.2045, -0.1182, -0.1618,  0.0513,  0.1632, -0.0135,\n",
      "         -0.2430, -0.2354]], requires_grad=True)\n",
      "Sesgos:\n",
      "Parameter containing:\n",
      "tensor([ 0.1892, -0.0865, -0.0885,  0.0902, -0.0346], requires_grad=True)\n",
      "Nombre de la capa: capa2\n",
      "Pesos:\n",
      "Parameter containing:\n",
      "tensor([[ 0.3551,  0.2183, -0.4091, -0.0641, -0.0446]], requires_grad=True)\n",
      "Sesgos:\n",
      "Parameter containing:\n",
      "tensor([0.0780], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0245, -0.3068,  0.2515,  0.3393, -0.3113,  0.1231,  0.2933,  0.0093],\n",
      "        [ 0.0419,  0.3087,  0.2597, -0.2594, -0.2269,  0.2603,  0.3263,  0.0289],\n",
      "        [ 0.2707,  0.0140, -0.2036, -0.1061, -0.2133, -0.1166,  0.3322, -0.2168],\n",
      "        [-0.3486,  0.3016,  0.0262,  0.1869,  0.0069,  0.1482,  0.1406,  0.3343],\n",
      "        [-0.1099, -0.1110, -0.0495,  0.2950, -0.1845,  0.3218,  0.2659,  0.0530],\n",
      "        [-0.1121, -0.1025,  0.2649,  0.2190, -0.0844,  0.1472, -0.2947,  0.0165],\n",
      "        [ 0.3525, -0.2114,  0.0086, -0.1476,  0.0035, -0.1090,  0.1460, -0.1979],\n",
      "        [ 0.3519, -0.2762, -0.0731,  0.2136,  0.0066, -0.2318, -0.1603,  0.1366],\n",
      "        [-0.3528,  0.0458, -0.0877,  0.0211, -0.0460,  0.3240, -0.1940, -0.1329],\n",
      "        [-0.0142, -0.0236,  0.2109,  0.0051,  0.1955, -0.3255,  0.2236, -0.1943]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define una red de ejemplo\n",
    "class MiRed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MiRed, self).__init__()\n",
    "        self.capa1 = nn.Linear(10, 5)\n",
    "        self.capa2 = nn.Linear(5, 1)\n",
    "\n",
    "mi_red = MiRed()\n",
    "\n",
    "# Recorre las capas de la red para encontrar las capas de tipo nn.Linear\n",
    "for nombre_capa, capa in mi_red.named_children():\n",
    "    if isinstance(capa, nn.Linear):\n",
    "        print(f\"Nombre de la capa: {nombre_capa}\")\n",
    "        pesos = capa.weight\n",
    "        sesgos = capa.bias\n",
    "        print(\"Pesos:\")\n",
    "        print(pesos)\n",
    "        print(\"Sesgos:\")\n",
    "        print(sesgos)\n",
    "\n",
    "\n",
    "print(getattr(red_A1,\"fc1\").weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3384,  0.2245, -0.3456, -0.2011, -0.0845,  0.3404,  0.2995,  0.2228,\n",
      "         0.3243, -0.1317, -0.1050,  0.1609,  0.0890, -0.0844,  0.1337,  0.3063,\n",
      "        -0.1552,  0.0420, -0.1662,  0.0392,  0.1885, -0.2366, -0.3159,  0.3315,\n",
      "         0.3000, -0.1660, -0.3179,  0.2154, -0.0440, -0.2754,  0.0224, -0.1549,\n",
      "        -0.3078, -0.1693, -0.2773, -0.1126, -0.3223, -0.0353, -0.2978, -0.2196,\n",
      "        -0.2321, -0.0659, -0.2351,  0.0627, -0.0241,  0.1985, -0.2821, -0.0762,\n",
      "        -0.3456, -0.0008, -0.2789, -0.3094, -0.0439,  0.2646, -0.2771,  0.0282,\n",
      "        -0.0918,  0.1136, -0.0824, -0.0621,  0.1819, -0.0361, -0.3385,  0.1276,\n",
      "         0.0392, -0.0365,  0.3273,  0.0628, -0.2072, -0.1392, -0.2651,  0.1502,\n",
      "         0.1415, -0.3427, -0.1770, -0.1052, -0.1880, -0.0107, -0.3439,  0.0895,\n",
      "         0.1800, -0.2447,  0.2382,  0.2502, -0.0451, -0.2995, -0.2923, -0.1688,\n",
      "        -0.0310, -0.2394, -0.2822, -0.1251,  0.1031,  0.0825, -0.0620, -0.0859,\n",
      "        -0.0413, -0.2790, -0.0023,  0.0978,  0.1896, -0.0409, -0.1772, -0.1224,\n",
      "         0.0793, -0.1707,  0.1481,  0.1716, -0.2311, -0.0913, -0.1849, -0.3034,\n",
      "         0.3085, -0.1986, -0.0969, -0.0592,  0.0324,  0.3119, -0.2050,  0.1246,\n",
      "        -0.0010, -0.0659, -0.0551,  0.1978,  0.1840, -0.2357, -0.2712,  0.3135,\n",
      "        -0.1787, -0.1829,  0.1291,  0.2156,  0.1232,  0.2838,  0.1863, -0.1175,\n",
      "         0.1915, -0.0967, -0.1753,  0.2939,  0.2629,  0.0005,  0.1210,  0.1957,\n",
      "        -0.0332, -0.1872,  0.1732, -0.1197, -0.0972, -0.0931,  0.2595, -0.1718,\n",
      "        -0.1901,  0.2852,  0.1356, -0.3149,  0.0414, -0.0680,  0.1266,  0.3041,\n",
      "         0.2558, -0.0628, -0.1305,  0.0512,  0.0695,  0.3061, -0.1593, -0.2816,\n",
      "        -0.2746, -0.2480,  0.2925, -0.2294,  0.3037,  0.2838,  0.2458,  0.2696,\n",
      "        -0.0330,  0.1599,  0.1751,  0.2699, -0.0902, -0.1603, -0.1476, -0.1692,\n",
      "         0.2066, -0.2716,  0.0182, -0.0048, -0.0480, -0.3118, -0.0249, -0.3072,\n",
      "         0.1703,  0.2826,  0.1377, -0.1166,  0.0528, -0.2554, -0.0267, -0.0603,\n",
      "        -0.2801,  0.0767, -0.0856, -0.0266,  0.1139,  0.0725,  0.1944,  0.0391,\n",
      "        -0.0096,  0.2062,  0.0369], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "class NARNN(nn.Module):\n",
    "    def __init__(self, parametros, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(NARNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,10)\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "        self.fc3 = nn.Linear(10,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))\n",
    "        x = tan_sigmoid(self.fc1(x))\n",
    "        x = F.logsigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "red_A1 = NARNN([], input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "_input=torch.randn(8)\n",
    "\n",
    "def func(*parametros):\n",
    "    index = 0\n",
    "    for capa, instancia in red_A1.named_children():\n",
    "        # print(f\"Nombre de la capa: {capa}\")\n",
    "        # print(f\"Tama単o del peso de la capa: {getattr(red_A1,capa).weight.size()}\")\n",
    "        # print(f\"Tama単o de la entrada: {parametros[index].size()}\")\n",
    "\n",
    "        # print(f\"Tama単o del sesgo de la capa: {getattr(red_A1,capa).bias.size()}\")\n",
    "        # print(f\"Tama単o de la entrada: {parametros[index+1].size()}\")\n",
    "        del getattr(red_A1,capa).weight\n",
    "        del getattr(red_A1,capa).bias\n",
    "        getattr(red_A1,capa).weight = nn.Parameter(parametros[index])\n",
    "        getattr(red_A1,capa).bias = nn.Parameter(parametros[index+1])\n",
    "        index = index + 2\n",
    "    # print(layer(_input))\n",
    "    # print(torch.zeros(len(_input),dtype=torch.long))\n",
    "    return criterion(red_A1(_input), torch.zeros_like(red_A1(_input)))\n",
    "\n",
    "w1 = torch.rand_like(red_A1.fc1.weight)\n",
    "b1 = torch.rand_like(red_A1.fc1.bias)\n",
    "w2 = torch.rand_like(red_A1.fc2.weight)\n",
    "b2 = torch.rand_like(red_A1.fc2.bias)\n",
    "w3 = torch.rand_like(red_A1.fc3.weight)\n",
    "b3 = torch.rand_like(red_A1.fc3.bias)\n",
    "#print([_.view(-1) for _ in red_A1.parameters()])\n",
    "# h = torch.autograd.functional.hessian(func,(w1,b1,w2,b2,w3,b3))\n",
    "\n",
    "print( torch.cat([_.view(-1) for _ in red_A1.parameters()], dim = 0))\n",
    "#print(h)\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     red_A1 = NARNN(parametros, input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "#     loss = criterion(red_A1(torch.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0])),torch.tensor([1]))#devuelve la perdida\n",
    "#     return loss\n",
    "\n",
    "# print(torch.autograd.functional.hessian(calcula_perdida, tuple([_ for _ in red_A1.parameters()])))\n",
    "\n",
    "_input=torch.randn(10)\n",
    "# layer = nn.Linear(3,4)\n",
    "# criterion=nn.MSELoss()\n",
    "\n",
    "# weight = layer.weight\n",
    "\n",
    "# class MiRed(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(MiRed, self).__init__()\n",
    "#         self.capa1 = nn.Linear(10, 5)\n",
    "#         self.capa2 = nn.Linear(5, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))\n",
    "#         x = tan_sigmoid(self.capa1(x))\n",
    "#         #print(y)\n",
    "#         #x = torch.sigmoid(self.fc1(x))\n",
    "#         #print(x)\n",
    "#         x = F.logsigmoid(self.capa2(x))\n",
    "#         return x\n",
    "\n",
    "# mi_red = MiRed()\n",
    "# weight = torch.rand_like(mi_red.capa1.weight)\n",
    "# bias = torch.rand_like(mi_red.capa1.bias)\n",
    "# # Acceso directo a los par叩metros usando 鱈ndices expl鱈citos\n",
    "# # parametros_capa1_peso = mi_red.capa1.weight\n",
    "# # parametros_capa1_bias = mi_red.capa1.bias\n",
    "# #print(weight)\n",
    "\n",
    "# def func(we,bi):\n",
    "#     del mi_red.capa1.weight\n",
    "#     del mi_red.capa1.bias\n",
    "#     mi_red.capa1.weight= we\n",
    "#     mi_red.capa1.bias = bi\n",
    "#     # print(layer(_input))\n",
    "#     # print(torch.zeros(len(_input),dtype=torch.long))\n",
    "#     return criterion(mi_red(_input), torch.zeros_like(mi_red(_input)))\n",
    "\n",
    "# print(torch.autograd.functional.hessian(func,(weight,bias)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NARNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(NARNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,10)\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "        self.fc3 = nn.Linear(10,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))\n",
    "        x = tan_sigmoid(self.fc1(x))\n",
    "        x = F.logsigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "red_A1 = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#        for parametro, entrada in zip(red_A1.parameters(),parametros):\n",
    "#               parametro.data = entrada\n",
    "#        loss = criterion(red_A1(torch.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0])),torch.tensor([1]))#devuelve la perdida\n",
    "#        return loss\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     for i, parametro in enumerate(red_A1.parameters()):\n",
    "#         parametro = nn.Parameter(parametros[i])\n",
    "#     loss = criterion(red_A1(torch.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0])),torch.tensor([1]))#devuelve la perdida\n",
    "#     return loss\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     with torch.no_grad():\n",
    "#         for i, parametro in enumerate(red_A1.parameters()):\n",
    "#             parametro.data = parametros[i].data\n",
    "#     loss = criterion(red_A1(torch.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0])),torch.tensor([1]))#devuelve la perdida\n",
    "#     return loss\n",
    "\n",
    "def calcula_perdida(*parametros):\n",
    "    with torch.no_grad():\n",
    "        for p, nuevo_p in zip(red_A1.parameters(), parametros):\n",
    "            p.copy_(nuevo_p)\n",
    "    loss = criterion(red_A1(torch.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0])),torch.tensor([1]))#devuelve la perdida\n",
    "    return loss\n",
    "\n",
    "print(torch.autograd.functional.hessian(calcula_perdida, tuple([_ for _ in red_A1.parameters()])))\n",
    "\n",
    "# e = []\n",
    "# for p in red_A1.parameters():\n",
    "#        e.append(torch.rand_like(p.data)) \n",
    "\n",
    "# #tuple()\n",
    "# #print(red_A1(torch.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0])))\n",
    "# a = [p.data for p in red_A1.parameters()]\n",
    "#print(calcula_perdida(a[0],a[1],a[2],a[3],a[4],a[5]))\n",
    "\n",
    "#print()\n",
    "#print(tuple([_.view(-1) for _ in red_A1.parameters()]))\n",
    "#print(tuple([_.view(-1) for _ in red_A1.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class NARNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "\n",
    "        super(NARNN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim,10)\n",
    "\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "\n",
    "        self.fc3 = nn.Linear(10,output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))\n",
    "\n",
    "        x = tan_sigmoid(self.fc1(x))\n",
    "\n",
    "        x = F.logsigmoid(self.fc2(x))\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def calculate_hessian(model, inputs, outputs, criterion):\n",
    "\n",
    "    # The loss and its gradients w.r.t. the parameters need to be computed.\n",
    "\n",
    "    loss = criterion(outputs, inputs)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # This vector will store the gradient of the loss w.r.t. each parameter.\n",
    "\n",
    "    grads = []\n",
    "\n",
    "    for param in model.parameters():\n",
    "\n",
    "        grads.append(param.grad.clone().detach().view(-1))\n",
    "\n",
    "\n",
    "    # This vector will store the diagonal elements of the Hessian matrix.\n",
    "\n",
    "    hessian_diag = []\n",
    "\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "\n",
    "        h = torch.zeros(param.numel(), param.numel())\n",
    "\n",
    "        grad_ii = grads[i][:, None] # Vector i at position i.\n",
    "\n",
    "        grad_jj = grads[i][None, :] # Vector i at position j.\n",
    "\n",
    "        h.as_strided(grad_ii.size(), (h.stride(0), h.stride(1) + h.size(1))).copy_(grad_ii)\n",
    "\n",
    "        h.as_strided(grad_jj.size(), (h.stride(0) + h.size(0), h.stride(1))).copy_(grad_jj)\n",
    "\n",
    "        hessian_diag.append(h.detach())\n",
    "\n",
    "\n",
    "    return hessian_diag\n",
    "\n",
    "\n",
    "# Initialize the model and criterion\n",
    "\n",
    "model = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Calculate the hessian\n",
    "\n",
    "hessian = calculate_hessian(model, torch.tensor([1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0]), torch.tensor([1]), criterion)\n",
    "\n",
    "print(hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuple([_.data for _ in red_A1.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos para los gr叩ficos\n",
    "x1 = [1, 2, 3, 4, 5]\n",
    "y1 = [10, 15, 7, 12, 9]\n",
    "\n",
    "x2 = [1, 2, 3, 4, 5]\n",
    "y2 = [5, 8, 12, 6, 10]\n",
    "\n",
    "x3 = [1, 2, 3, 4, 5]\n",
    "y3 = [9, 12, 6, 10, 8]\n",
    "\n",
    "x4 = [1, 2, 3, 4, 5]\n",
    "y4 = [8, 7, 5, 11, 14]\n",
    "\n",
    "x5 = [1, 2, 3, 4, 5]\n",
    "y5 = [6, 9, 11, 15, 12]\n",
    "\n",
    "x6 = [1, 2, 3, 4, 5]\n",
    "y6 = [13, 10, 14, 8, 5]\n",
    "\n",
    "# Crear una figura con 2 filas y 3 columnas de subtramas\n",
    "plt.figure(figsize=(12, 8))  # Ajusta el tama単o de la figura\n",
    "\n",
    "# Subtrama 1\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(x1, y1)\n",
    "plt.title('Gr叩fico 1')\n",
    "\n",
    "# Subtrama 2\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(x2, y2)\n",
    "plt.title('Gr叩fico 2')\n",
    "\n",
    "# Subtrama 3\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(x3, y3)\n",
    "plt.title('Gr叩fico 3')\n",
    "\n",
    "# Subtrama 4\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(x4, y4)\n",
    "plt.title('Gr叩fico 4')\n",
    "\n",
    "# Subtrama 5\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(x5, y5)\n",
    "plt.title('Gr叩fico 5')\n",
    "\n",
    "# Subtrama 6\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot(x6, y6)\n",
    "plt.title('Gr叩fico 6')\n",
    "\n",
    "# Ajusta el espacio entre las subtramas para evitar superposiciones\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in red_A1.parameters():\n",
    "    print(\"param: \" + str(p.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_MSEloss(params):\n",
    "    c = 0\n",
    "    for param in red_A1.parameters():\n",
    "        param = params[c]\n",
    "        print(param)\n",
    "        c = c + 1\n",
    "\n",
    "for j in red_A1.parameters():\n",
    "    print(j)\n",
    "\n",
    "my_MSEloss([torch.zeros_like(tensor) for tensor in red_A1.parameters()])\n",
    "#for i in red_A1.parameters():\n",
    " #   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(red_A1.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.rand(2, 2))\n",
    "\n",
    "for i in torch.rand(2, 2):\n",
    "    print(i)\n",
    "    print(type(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor([1.0, -1.0, 2.0]))\n",
    "        self.b = torch.nn.Parameter(torch.tensor([2.0, -2.0, -1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = (x * self.a.unsqueeze(0) ** 3 + x * self.b.unsqueeze(0) ** 3).sum(dim=1)\n",
    "        return output\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = torch.tensor([1.0, 2.0])\n",
    "net = Net()\n",
    "\n",
    "def compute_z(*net_parameters):\n",
    "    # Is there a proper way to set the parameters that works with hessian?\n",
    "    for p_src, p_dst in zip(net_parameters, net.parameters()):\n",
    "        p_dst.data = p_src.data\n",
    "\n",
    "    output = net(x)\n",
    "    z = ((output - y)**2).mean()\n",
    "    return z\n",
    "\n",
    "# strict=True raises exception, allowing non-strict for demonstration\n",
    "hessians = torch.autograd.functional.hessian(compute_z, tuple(net.parameters()))\n",
    "\n",
    "param_names = [n for n, _ in net.named_parameters()]\n",
    "for d_name, d_hessians in zip(param_names, hessians):\n",
    "    for dd_name, dd_hessian in zip(param_names, d_hessians):\n",
    "        print(f'dz/d{dd_name}d{d_name} = \\n{dd_hessian}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(red.parameters(), lr=0.1, momentum=0.4)#,maximize=True)\n",
    "#optimizer.zero_grad()\n",
    "output = red_A1(torch.Tensor([1,2,3,4,5,6,7,8]))\n",
    "target = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "print(type(criterion))\n",
    "print(type(loss.grad_fn))\n",
    "\n",
    "print(output)\n",
    "print(type(output.grad_fn))\n",
    "\n",
    "t = []\n",
    "\n",
    "for c in red_A1.parameters():\n",
    "    print(type(c.data))\n",
    "    t.append(c.data)\n",
    "\n",
    "#torch.cat(,dim = 0)\n",
    "# print(t)\n",
    "\n",
    "h = torch.autograd.functional.hessian(loss.grad_fn, torch.rand(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(loss)\n",
    "loss.backward()\n",
    "h = torch.autograd.functional.hessian(loss)#calcula el hessiana de la funcion de perdida\n",
    "h+ lambda \n",
    "print(loss.backward())\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
