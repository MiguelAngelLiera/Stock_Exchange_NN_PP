{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5., 6., 7., 8.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, h, w):\n",
    "        super(Net, self).__init__()\n",
    "        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.f2 = torch.nn.Linear(32 * h * w, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.f2(x)\n",
    "        return x\n",
    "\n",
    "class NARNN(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(NARNN, self).__init__()\n",
    "        #self.hidden_dim = hidden_dim\n",
    "        #self.num_layers = num_layers\n",
    "        #self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)#capa lstm\n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim)#capa lineal\n",
    "        self.fc1 = nn.Linear(input_dim,10)\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "        self.fc3 = nn.Linear(10,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()#Crea tensores con las dimensiones especificadas\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc1(out[:, -1, :]) \n",
    "        return out\"\"\"\n",
    "        tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))\n",
    "        x = tan_sigmoid(self.fc1(x))\n",
    "        #print(y)\n",
    "        #x = torch.sigmoid(self.fc1(x))\n",
    "        #print(x)\n",
    "        x = F.logsigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def fun(a, b, c, d):\n",
    "    p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]\n",
    "    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)\n",
    "    y = torch.randint(0, 5, [8])\n",
    "    x = F.conv2d(x, p[0], p[1], 1, 1)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.linear(x, p[2], p[3])\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    return loss\n",
    "\n",
    "red = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "input = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "\n",
    "print(input)\n",
    "entrada = input #la entrada se da como un parametro global\n",
    "salida_esperada = torch.tensor([-0.0834]) #lo mismo para la salida esperada\n",
    "λ = 0.1\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     #red.fc1.weight.data = \n",
    "#     params = []\n",
    "#     i=0\n",
    "#     for param in parametros:\n",
    "#         params.append(param)\n",
    "#         #print(param)\n",
    "#     for r_param in red.parameters():\n",
    "#         params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "#         r_param = params[i]\n",
    "#         i = i+1\n",
    "#     #print([b for b in red.parameters()])\n",
    "#     salida = red(entrada)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     return criterion(salida,salida_esperada)\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     #red.fc1.weight.data = \n",
    "#     params = []\n",
    "#     i=0\n",
    "#     #parametros = list(parametros)\n",
    "#     for param in parametros:\n",
    "#         params.append(param)\n",
    "#         print(param)\n",
    "#     for r_param in red.parameters():\n",
    "#         params[i] = params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "#         # r_param = params[i]\n",
    "#         i = i+1\n",
    "#     #print([b for b in red.parameters()]\n",
    "#     l1 = F.linear(entrada,params[0],params[1])\n",
    "#     #print(params[0])\n",
    "#     #print(params[1])\n",
    "#     #print(\"Entradal1: \" + str(l1))\n",
    "#     l2 = F.linear(l1,params[2],params[3])\n",
    "#     #print(\"Entradal2: \" + str(l2))\n",
    "#     salida = F.linear(l2,params[4],params[5])\n",
    "#     #print(\"Salida: \" + str(salida))\n",
    "\n",
    "#     #salida = (entrada)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     return criterion(salida,salida_esperada)\n",
    "\n",
    "def calcula_perdida(*parametros):\n",
    "    #red.fc1.weight.data = \n",
    "    params = []\n",
    "    n_params = []\n",
    "    i=0\n",
    "    n_dim = 0\n",
    "    #parametros = list(parametros)\n",
    "    for param in parametros:\n",
    "        params.append(param)#recibiria un solo tensor con todos los pesos\n",
    "    for r_param in red.parameters():\n",
    "        p_partida = n_dim\n",
    "        print(\"shape: \" + str(r_param.shape))\n",
    "        n_dim = r_param.size(0)*(r_param.size(1) if r_param.dim() == 2 else 1) + p_partida#se obtiene la dimension del primer conjunto de pparametros de la red\n",
    "        print(\"n_dim: \" + str(n_dim) + \" p_partida: \" + str(p_partida))\n",
    "        #print(\"primer vector: \" + str(params[0][p_partida:n_dim]))\n",
    "        n_params.append(params[0][p_partida:n_dim])\n",
    "        print(\"primer vector: \" + str(n_params[i]))\n",
    "        n_params[i] = n_params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "        print(\"segundo vector: \" + str(n_params[i]))\n",
    "        i = i+1\n",
    "    #print([b for b in red.parameters()]\n",
    "    l1 = F.linear(entrada,n_params[0],n_params[1])\n",
    "    #print(params[0])\n",
    "    #print(params[1])\n",
    "    #print(\"Entradal1: \" + str(l1))\n",
    "    l2 = F.linear(l1,n_params[2],n_params[3])\n",
    "    #print(\"Entradal2: \" + str(l2))\n",
    "    salida = F.linear(l2,n_params[4],n_params[5])\n",
    "    #print(\"Salida: \" + str(salida))\n",
    "\n",
    "    #salida = (entrada)\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(salida,salida_esperada)\n",
    "\n",
    "\n",
    "#print([b for b in red.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([[0.5362, 0.9088, 0.9564, 0.1839, 0.2225],\n",
      "        [0.9121, 0.2969, 0.3343, 0.4736, 0.2443]])\n",
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n",
      "1\n",
      "tensor([[3.7634, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 2.6036, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.3099, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 3.8268, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9504, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.8396, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.9074, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 4.6816,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         3.4323]])\n"
     ]
    }
   ],
   "source": [
    "#torch.rand(2,2)\n",
    "a= torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "print(a.dim())\n",
    "print(torch.rand(2,5))\n",
    "print(a.view(2,5))\n",
    "def pow_reducer(x):\n",
    "    return x.pow(3).sum()\n",
    "inputs = torch.rand(10)\n",
    "print(inputs.dim())\n",
    "print(torch.autograd.functional.hessian(pow_reducer, inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([10, 8])\n",
      "n_dim: 80 p_partida: 0\n",
      "primer vector: tensor([-0.1959,  0.3342, -0.2366,  0.2364, -0.1655,  0.3015, -0.3058, -0.1338,\n",
      "         0.1965,  0.1493, -0.1877,  0.3120, -0.1346,  0.2856, -0.1745, -0.0723,\n",
      "        -0.1171, -0.2553, -0.2782, -0.2398, -0.1057,  0.0910,  0.3203, -0.0495,\n",
      "        -0.0040,  0.3270,  0.0104, -0.2153, -0.1081,  0.1247,  0.3139,  0.2591,\n",
      "         0.3039, -0.2032,  0.1952,  0.3196,  0.1710,  0.2821,  0.3091,  0.2921,\n",
      "         0.2477, -0.2997, -0.3333, -0.0167, -0.1838,  0.1239,  0.0974, -0.2778,\n",
      "        -0.1398,  0.3071, -0.0106, -0.0127,  0.0923, -0.0806,  0.1086,  0.0280,\n",
      "         0.0204,  0.0151,  0.0727, -0.0670,  0.0105, -0.0888,  0.3277, -0.2437,\n",
      "         0.2420,  0.0282,  0.3534,  0.2636, -0.0675, -0.2400,  0.1264, -0.2758,\n",
      "        -0.0225, -0.0685,  0.2602, -0.1006, -0.0839,  0.0710,  0.2769,  0.3441],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[-0.1959,  0.3342, -0.2366,  0.2364, -0.1655,  0.3015, -0.3058, -0.1338],\n",
      "        [ 0.1965,  0.1493, -0.1877,  0.3120, -0.1346,  0.2856, -0.1745, -0.0723],\n",
      "        [-0.1171, -0.2553, -0.2782, -0.2398, -0.1057,  0.0910,  0.3203, -0.0495],\n",
      "        [-0.0040,  0.3270,  0.0104, -0.2153, -0.1081,  0.1247,  0.3139,  0.2591],\n",
      "        [ 0.3039, -0.2032,  0.1952,  0.3196,  0.1710,  0.2821,  0.3091,  0.2921],\n",
      "        [ 0.2477, -0.2997, -0.3333, -0.0167, -0.1838,  0.1239,  0.0974, -0.2778],\n",
      "        [-0.1398,  0.3071, -0.0106, -0.0127,  0.0923, -0.0806,  0.1086,  0.0280],\n",
      "        [ 0.0204,  0.0151,  0.0727, -0.0670,  0.0105, -0.0888,  0.3277, -0.2437],\n",
      "        [ 0.2420,  0.0282,  0.3534,  0.2636, -0.0675, -0.2400,  0.1264, -0.2758],\n",
      "        [-0.0225, -0.0685,  0.2602, -0.1006, -0.0839,  0.0710,  0.2769,  0.3441]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10])\n",
      "n_dim: 90 p_partida: 80\n",
      "primer vector: tensor([-0.2950,  0.2946, -0.0880, -0.1270, -0.2494, -0.1489, -0.1692, -0.1555,\n",
      "         0.3483,  0.2124], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.2950,  0.2946, -0.0880, -0.1270, -0.2494, -0.1489, -0.1692, -0.1555,\n",
      "         0.3483,  0.2124], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10, 10])\n",
      "n_dim: 190 p_partida: 90\n",
      "primer vector: tensor([ 0.0734,  0.0658,  0.1918, -0.0702, -0.1767, -0.0031, -0.2320, -0.0169,\n",
      "         0.2656,  0.1041,  0.2682,  0.3070,  0.3021, -0.1436, -0.2765, -0.1225,\n",
      "         0.3027,  0.0832, -0.0436, -0.1715,  0.2098,  0.2851,  0.2126,  0.0897,\n",
      "        -0.2616, -0.1589,  0.2902, -0.0735, -0.0834,  0.0914,  0.2437,  0.2614,\n",
      "         0.1611, -0.1304, -0.2182, -0.1916,  0.2355,  0.1264, -0.1097,  0.0408,\n",
      "         0.2963,  0.2657, -0.2862,  0.1130,  0.0150, -0.1611,  0.0264, -0.1675,\n",
      "        -0.1295,  0.1687,  0.0572,  0.2195,  0.0564, -0.2699, -0.0400, -0.1613,\n",
      "         0.1684, -0.2682,  0.0019, -0.1613, -0.2830, -0.2527, -0.0454,  0.0130,\n",
      "         0.3036,  0.1458,  0.1505,  0.2554, -0.1458, -0.0614,  0.0754, -0.0837,\n",
      "        -0.0060, -0.0615, -0.0065,  0.0996, -0.2885, -0.1138,  0.2507, -0.1533,\n",
      "         0.0178,  0.2563,  0.2272,  0.0023, -0.2670, -0.0748,  0.2164,  0.1871,\n",
      "         0.1706, -0.2724,  0.0367,  0.1666, -0.2059, -0.2357, -0.0894,  0.0122,\n",
      "         0.0968, -0.0287, -0.0909, -0.1529], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[ 0.0734,  0.0658,  0.1918, -0.0702, -0.1767, -0.0031, -0.2320, -0.0169,\n",
      "          0.2656,  0.1041],\n",
      "        [ 0.2682,  0.3070,  0.3021, -0.1436, -0.2765, -0.1225,  0.3027,  0.0832,\n",
      "         -0.0436, -0.1715],\n",
      "        [ 0.2098,  0.2851,  0.2126,  0.0897, -0.2616, -0.1589,  0.2902, -0.0735,\n",
      "         -0.0834,  0.0914],\n",
      "        [ 0.2437,  0.2614,  0.1611, -0.1304, -0.2182, -0.1916,  0.2355,  0.1264,\n",
      "         -0.1097,  0.0408],\n",
      "        [ 0.2963,  0.2657, -0.2862,  0.1130,  0.0150, -0.1611,  0.0264, -0.1675,\n",
      "         -0.1295,  0.1687],\n",
      "        [ 0.0572,  0.2195,  0.0564, -0.2699, -0.0400, -0.1613,  0.1684, -0.2682,\n",
      "          0.0019, -0.1613],\n",
      "        [-0.2830, -0.2527, -0.0454,  0.0130,  0.3036,  0.1458,  0.1505,  0.2554,\n",
      "         -0.1458, -0.0614],\n",
      "        [ 0.0754, -0.0837, -0.0060, -0.0615, -0.0065,  0.0996, -0.2885, -0.1138,\n",
      "          0.2507, -0.1533],\n",
      "        [ 0.0178,  0.2563,  0.2272,  0.0023, -0.2670, -0.0748,  0.2164,  0.1871,\n",
      "          0.1706, -0.2724],\n",
      "        [ 0.0367,  0.1666, -0.2059, -0.2357, -0.0894,  0.0122,  0.0968, -0.0287,\n",
      "         -0.0909, -0.1529]], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10])\n",
      "n_dim: 200 p_partida: 190\n",
      "primer vector: tensor([-0.0118, -0.1141, -0.2869,  0.0980, -0.2533,  0.2106,  0.0219, -0.1774,\n",
      "         0.0805, -0.3091], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.0118, -0.1141, -0.2869,  0.0980, -0.2533,  0.2106,  0.0219, -0.1774,\n",
      "         0.0805, -0.3091], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([1, 10])\n",
      "n_dim: 210 p_partida: 200\n",
      "primer vector: tensor([ 0.0844, -0.2676,  0.3094, -0.1025,  0.3043,  0.3087,  0.3010,  0.0076,\n",
      "         0.2309,  0.0743], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[ 0.0844, -0.2676,  0.3094, -0.1025,  0.3043,  0.3087,  0.3010,  0.0076,\n",
      "          0.2309,  0.0743]], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([1])\n",
      "n_dim: 211 p_partida: 210\n",
      "primer vector: tensor([-0.2795], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.2795], grad_fn=<ViewBackward0>)\n",
      "tensor(0.1300, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "entrada = input\n",
    "#print(red.fc1.weight.data)\n",
    "#red.fc1.weight.data = torch.zeros((10,8))\n",
    "#print(red.fc1.weight.data)\n",
    "#print(torch.tensor([[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8]]))\n",
    "#print(red.fc2.weight.data)\n",
    "#print(red.fc3.weight.data)\n",
    "v = [_.view(-1) for _ in red.parameters()]\n",
    "#for i in v:\n",
    " #   print(i)\n",
    "v = torch.cat([_.view(-1) for _ in red.parameters()], dim = 0)#concatena los paremetros de la red en un solo vector unidimensional\n",
    "#print(v)\n",
    "p = calcula_perdida(v)\n",
    "#print(len(v))\n",
    "#p = calcula_perdida(v[0],v[1],v[2],v[3],v[4],v[5])\n",
    "#print(red(entrada))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "shape: torch.Size([10, 8])\n",
      "n_dim: 80 p_partida: 0\n",
      "primer vector: tensor([-0.1959,  0.3342, -0.2366,  0.2364, -0.1655,  0.3015, -0.3058, -0.1338,\n",
      "         0.1965,  0.1493, -0.1877,  0.3120, -0.1346,  0.2856, -0.1745, -0.0723,\n",
      "        -0.1171, -0.2553, -0.2782, -0.2398, -0.1057,  0.0910,  0.3203, -0.0495,\n",
      "        -0.0040,  0.3270,  0.0104, -0.2153, -0.1081,  0.1247,  0.3139,  0.2591,\n",
      "         0.3039, -0.2032,  0.1952,  0.3196,  0.1710,  0.2821,  0.3091,  0.2921,\n",
      "         0.2477, -0.2997, -0.3333, -0.0167, -0.1838,  0.1239,  0.0974, -0.2778,\n",
      "        -0.1398,  0.3071, -0.0106, -0.0127,  0.0923, -0.0806,  0.1086,  0.0280,\n",
      "         0.0204,  0.0151,  0.0727, -0.0670,  0.0105, -0.0888,  0.3277, -0.2437,\n",
      "         0.2420,  0.0282,  0.3534,  0.2636, -0.0675, -0.2400,  0.1264, -0.2758,\n",
      "        -0.0225, -0.0685,  0.2602, -0.1006, -0.0839,  0.0710,  0.2769,  0.3441],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[-0.1959,  0.3342, -0.2366,  0.2364, -0.1655,  0.3015, -0.3058, -0.1338],\n",
      "        [ 0.1965,  0.1493, -0.1877,  0.3120, -0.1346,  0.2856, -0.1745, -0.0723],\n",
      "        [-0.1171, -0.2553, -0.2782, -0.2398, -0.1057,  0.0910,  0.3203, -0.0495],\n",
      "        [-0.0040,  0.3270,  0.0104, -0.2153, -0.1081,  0.1247,  0.3139,  0.2591],\n",
      "        [ 0.3039, -0.2032,  0.1952,  0.3196,  0.1710,  0.2821,  0.3091,  0.2921],\n",
      "        [ 0.2477, -0.2997, -0.3333, -0.0167, -0.1838,  0.1239,  0.0974, -0.2778],\n",
      "        [-0.1398,  0.3071, -0.0106, -0.0127,  0.0923, -0.0806,  0.1086,  0.0280],\n",
      "        [ 0.0204,  0.0151,  0.0727, -0.0670,  0.0105, -0.0888,  0.3277, -0.2437],\n",
      "        [ 0.2420,  0.0282,  0.3534,  0.2636, -0.0675, -0.2400,  0.1264, -0.2758],\n",
      "        [-0.0225, -0.0685,  0.2602, -0.1006, -0.0839,  0.0710,  0.2769,  0.3441]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10])\n",
      "n_dim: 90 p_partida: 80\n",
      "primer vector: tensor([-0.2950,  0.2946, -0.0880, -0.1270, -0.2494, -0.1489, -0.1692, -0.1555,\n",
      "         0.3483,  0.2124], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.2950,  0.2946, -0.0880, -0.1270, -0.2494, -0.1489, -0.1692, -0.1555,\n",
      "         0.3483,  0.2124], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10, 10])\n",
      "n_dim: 190 p_partida: 90\n",
      "primer vector: tensor([ 0.0734,  0.0658,  0.1918, -0.0702, -0.1767, -0.0031, -0.2320, -0.0169,\n",
      "         0.2656,  0.1041,  0.2682,  0.3070,  0.3021, -0.1436, -0.2765, -0.1225,\n",
      "         0.3027,  0.0832, -0.0436, -0.1715,  0.2098,  0.2851,  0.2126,  0.0897,\n",
      "        -0.2616, -0.1589,  0.2902, -0.0735, -0.0834,  0.0914,  0.2437,  0.2614,\n",
      "         0.1611, -0.1304, -0.2182, -0.1916,  0.2355,  0.1264, -0.1097,  0.0408,\n",
      "         0.2963,  0.2657, -0.2862,  0.1130,  0.0150, -0.1611,  0.0264, -0.1675,\n",
      "        -0.1295,  0.1687,  0.0572,  0.2195,  0.0564, -0.2699, -0.0400, -0.1613,\n",
      "         0.1684, -0.2682,  0.0019, -0.1613, -0.2830, -0.2527, -0.0454,  0.0130,\n",
      "         0.3036,  0.1458,  0.1505,  0.2554, -0.1458, -0.0614,  0.0754, -0.0837,\n",
      "        -0.0060, -0.0615, -0.0065,  0.0996, -0.2885, -0.1138,  0.2507, -0.1533,\n",
      "         0.0178,  0.2563,  0.2272,  0.0023, -0.2670, -0.0748,  0.2164,  0.1871,\n",
      "         0.1706, -0.2724,  0.0367,  0.1666, -0.2059, -0.2357, -0.0894,  0.0122,\n",
      "         0.0968, -0.0287, -0.0909, -0.1529], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[ 0.0734,  0.0658,  0.1918, -0.0702, -0.1767, -0.0031, -0.2320, -0.0169,\n",
      "          0.2656,  0.1041],\n",
      "        [ 0.2682,  0.3070,  0.3021, -0.1436, -0.2765, -0.1225,  0.3027,  0.0832,\n",
      "         -0.0436, -0.1715],\n",
      "        [ 0.2098,  0.2851,  0.2126,  0.0897, -0.2616, -0.1589,  0.2902, -0.0735,\n",
      "         -0.0834,  0.0914],\n",
      "        [ 0.2437,  0.2614,  0.1611, -0.1304, -0.2182, -0.1916,  0.2355,  0.1264,\n",
      "         -0.1097,  0.0408],\n",
      "        [ 0.2963,  0.2657, -0.2862,  0.1130,  0.0150, -0.1611,  0.0264, -0.1675,\n",
      "         -0.1295,  0.1687],\n",
      "        [ 0.0572,  0.2195,  0.0564, -0.2699, -0.0400, -0.1613,  0.1684, -0.2682,\n",
      "          0.0019, -0.1613],\n",
      "        [-0.2830, -0.2527, -0.0454,  0.0130,  0.3036,  0.1458,  0.1505,  0.2554,\n",
      "         -0.1458, -0.0614],\n",
      "        [ 0.0754, -0.0837, -0.0060, -0.0615, -0.0065,  0.0996, -0.2885, -0.1138,\n",
      "          0.2507, -0.1533],\n",
      "        [ 0.0178,  0.2563,  0.2272,  0.0023, -0.2670, -0.0748,  0.2164,  0.1871,\n",
      "          0.1706, -0.2724],\n",
      "        [ 0.0367,  0.1666, -0.2059, -0.2357, -0.0894,  0.0122,  0.0968, -0.0287,\n",
      "         -0.0909, -0.1529]], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10])\n",
      "n_dim: 200 p_partida: 190\n",
      "primer vector: tensor([-0.0118, -0.1141, -0.2869,  0.0980, -0.2533,  0.2106,  0.0219, -0.1774,\n",
      "         0.0805, -0.3091], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.0118, -0.1141, -0.2869,  0.0980, -0.2533,  0.2106,  0.0219, -0.1774,\n",
      "         0.0805, -0.3091], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([1, 10])\n",
      "n_dim: 210 p_partida: 200\n",
      "primer vector: tensor([ 0.0844, -0.2676,  0.3094, -0.1025,  0.3043,  0.3087,  0.3010,  0.0076,\n",
      "         0.2309,  0.0743], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[ 0.0844, -0.2676,  0.3094, -0.1025,  0.3043,  0.3087,  0.3010,  0.0076,\n",
      "          0.2309,  0.0743]], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([1])\n",
      "n_dim: 211 p_partida: 210\n",
      "primer vector: tensor([-0.2795], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.2795], grad_fn=<ViewBackward0>)\n",
      "2\n",
      "torch.Size([211, 211])\n"
     ]
    }
   ],
   "source": [
    "print(v.dim())\n",
    "# class Net(Module):\n",
    "#     def __init__(self, h, w):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)\n",
    "#         self.f2 = torch.nn.Linear(32 * h * w, 5)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.c1(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.f2(x)\n",
    "#         return x\n",
    "\n",
    "# def haha(a, b, c, d):\n",
    "#     p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]\n",
    "#     x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)\n",
    "#     y = torch.randint(0, 5, [8])\n",
    "#     x = F.conv2d(x, p[0], p[1], 1, 1)\n",
    "#     x = x.view(x.size(0), -1)\n",
    "#     x = F.linear(x, p[2], p[3])\n",
    "#     loss = F.cross_entropy(x, y)\n",
    "#     return loss\n",
    "h = torch.autograd.functional.hessian(calcula_perdida, v)\n",
    "#print(len(list(a)))\n",
    "#print(len(list(a)[0]))\n",
    "print(h.dim())\n",
    "print(h.shape)\n",
    "\n",
    "#print(h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([10, 8])\n",
      "n_dim: 80 p_partida: 0\n",
      "primer vector: tensor([-0.1959,  0.3342, -0.2366,  0.2364, -0.1655,  0.3015, -0.3058, -0.1338,\n",
      "         0.1965,  0.1493, -0.1877,  0.3120, -0.1346,  0.2856, -0.1745, -0.0723,\n",
      "        -0.1171, -0.2553, -0.2782, -0.2398, -0.1057,  0.0910,  0.3203, -0.0495,\n",
      "        -0.0040,  0.3270,  0.0104, -0.2153, -0.1081,  0.1247,  0.3139,  0.2591,\n",
      "         0.3039, -0.2032,  0.1952,  0.3196,  0.1710,  0.2821,  0.3091,  0.2921,\n",
      "         0.2477, -0.2997, -0.3333, -0.0167, -0.1838,  0.1239,  0.0974, -0.2778,\n",
      "        -0.1398,  0.3071, -0.0106, -0.0127,  0.0923, -0.0806,  0.1086,  0.0280,\n",
      "         0.0204,  0.0151,  0.0727, -0.0670,  0.0105, -0.0888,  0.3277, -0.2437,\n",
      "         0.2420,  0.0282,  0.3534,  0.2636, -0.0675, -0.2400,  0.1264, -0.2758,\n",
      "        -0.0225, -0.0685,  0.2602, -0.1006, -0.0839,  0.0710,  0.2769,  0.3441],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[-0.1959,  0.3342, -0.2366,  0.2364, -0.1655,  0.3015, -0.3058, -0.1338],\n",
      "        [ 0.1965,  0.1493, -0.1877,  0.3120, -0.1346,  0.2856, -0.1745, -0.0723],\n",
      "        [-0.1171, -0.2553, -0.2782, -0.2398, -0.1057,  0.0910,  0.3203, -0.0495],\n",
      "        [-0.0040,  0.3270,  0.0104, -0.2153, -0.1081,  0.1247,  0.3139,  0.2591],\n",
      "        [ 0.3039, -0.2032,  0.1952,  0.3196,  0.1710,  0.2821,  0.3091,  0.2921],\n",
      "        [ 0.2477, -0.2997, -0.3333, -0.0167, -0.1838,  0.1239,  0.0974, -0.2778],\n",
      "        [-0.1398,  0.3071, -0.0106, -0.0127,  0.0923, -0.0806,  0.1086,  0.0280],\n",
      "        [ 0.0204,  0.0151,  0.0727, -0.0670,  0.0105, -0.0888,  0.3277, -0.2437],\n",
      "        [ 0.2420,  0.0282,  0.3534,  0.2636, -0.0675, -0.2400,  0.1264, -0.2758],\n",
      "        [-0.0225, -0.0685,  0.2602, -0.1006, -0.0839,  0.0710,  0.2769,  0.3441]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10])\n",
      "n_dim: 90 p_partida: 80\n",
      "primer vector: tensor([-0.2950,  0.2946, -0.0880, -0.1270, -0.2494, -0.1489, -0.1692, -0.1555,\n",
      "         0.3483,  0.2124], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.2950,  0.2946, -0.0880, -0.1270, -0.2494, -0.1489, -0.1692, -0.1555,\n",
      "         0.3483,  0.2124], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10, 10])\n",
      "n_dim: 190 p_partida: 90\n",
      "primer vector: tensor([ 0.0734,  0.0658,  0.1918, -0.0702, -0.1767, -0.0031, -0.2320, -0.0169,\n",
      "         0.2656,  0.1041,  0.2682,  0.3070,  0.3021, -0.1436, -0.2765, -0.1225,\n",
      "         0.3027,  0.0832, -0.0436, -0.1715,  0.2098,  0.2851,  0.2126,  0.0897,\n",
      "        -0.2616, -0.1589,  0.2902, -0.0735, -0.0834,  0.0914,  0.2437,  0.2614,\n",
      "         0.1611, -0.1304, -0.2182, -0.1916,  0.2355,  0.1264, -0.1097,  0.0408,\n",
      "         0.2963,  0.2657, -0.2862,  0.1130,  0.0150, -0.1611,  0.0264, -0.1675,\n",
      "        -0.1295,  0.1687,  0.0572,  0.2195,  0.0564, -0.2699, -0.0400, -0.1613,\n",
      "         0.1684, -0.2682,  0.0019, -0.1613, -0.2830, -0.2527, -0.0454,  0.0130,\n",
      "         0.3036,  0.1458,  0.1505,  0.2554, -0.1458, -0.0614,  0.0754, -0.0837,\n",
      "        -0.0060, -0.0615, -0.0065,  0.0996, -0.2885, -0.1138,  0.2507, -0.1533,\n",
      "         0.0178,  0.2563,  0.2272,  0.0023, -0.2670, -0.0748,  0.2164,  0.1871,\n",
      "         0.1706, -0.2724,  0.0367,  0.1666, -0.2059, -0.2357, -0.0894,  0.0122,\n",
      "         0.0968, -0.0287, -0.0909, -0.1529], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[ 0.0734,  0.0658,  0.1918, -0.0702, -0.1767, -0.0031, -0.2320, -0.0169,\n",
      "          0.2656,  0.1041],\n",
      "        [ 0.2682,  0.3070,  0.3021, -0.1436, -0.2765, -0.1225,  0.3027,  0.0832,\n",
      "         -0.0436, -0.1715],\n",
      "        [ 0.2098,  0.2851,  0.2126,  0.0897, -0.2616, -0.1589,  0.2902, -0.0735,\n",
      "         -0.0834,  0.0914],\n",
      "        [ 0.2437,  0.2614,  0.1611, -0.1304, -0.2182, -0.1916,  0.2355,  0.1264,\n",
      "         -0.1097,  0.0408],\n",
      "        [ 0.2963,  0.2657, -0.2862,  0.1130,  0.0150, -0.1611,  0.0264, -0.1675,\n",
      "         -0.1295,  0.1687],\n",
      "        [ 0.0572,  0.2195,  0.0564, -0.2699, -0.0400, -0.1613,  0.1684, -0.2682,\n",
      "          0.0019, -0.1613],\n",
      "        [-0.2830, -0.2527, -0.0454,  0.0130,  0.3036,  0.1458,  0.1505,  0.2554,\n",
      "         -0.1458, -0.0614],\n",
      "        [ 0.0754, -0.0837, -0.0060, -0.0615, -0.0065,  0.0996, -0.2885, -0.1138,\n",
      "          0.2507, -0.1533],\n",
      "        [ 0.0178,  0.2563,  0.2272,  0.0023, -0.2670, -0.0748,  0.2164,  0.1871,\n",
      "          0.1706, -0.2724],\n",
      "        [ 0.0367,  0.1666, -0.2059, -0.2357, -0.0894,  0.0122,  0.0968, -0.0287,\n",
      "         -0.0909, -0.1529]], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([10])\n",
      "n_dim: 200 p_partida: 190\n",
      "primer vector: tensor([-0.0118, -0.1141, -0.2869,  0.0980, -0.2533,  0.2106,  0.0219, -0.1774,\n",
      "         0.0805, -0.3091], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.0118, -0.1141, -0.2869,  0.0980, -0.2533,  0.2106,  0.0219, -0.1774,\n",
      "         0.0805, -0.3091], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([1, 10])\n",
      "n_dim: 210 p_partida: 200\n",
      "primer vector: tensor([ 0.0844, -0.2676,  0.3094, -0.1025,  0.3043,  0.3087,  0.3010,  0.0076,\n",
      "         0.2309,  0.0743], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([[ 0.0844, -0.2676,  0.3094, -0.1025,  0.3043,  0.3087,  0.3010,  0.0076,\n",
      "          0.2309,  0.0743]], grad_fn=<ViewBackward0>)\n",
      "shape: torch.Size([1])\n",
      "n_dim: 211 p_partida: 210\n",
      "primer vector: tensor([-0.2795], grad_fn=<SliceBackward0>)\n",
      "segundo vector: tensor([-0.2795], grad_fn=<ViewBackward0>)\n",
      "torch.Size([211])\n"
     ]
    }
   ],
   "source": [
    "grad_f = torch.autograd.grad(calcula_perdida(v), v)[0]\n",
    "print(grad_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2990e-05,  6.7240e-05,  1.0042e-04,  1.3441e-04,  1.6783e-04,\n",
      "          2.0080e-04,  2.3457e-04,  2.6874e-04, -1.2525e-05, -2.5565e-05,\n",
      "         -3.9203e-05, -5.1367e-05, -6.4396e-05, -7.6674e-05, -8.8923e-05,\n",
      "         -1.0275e-04,  1.1452e-05,  2.2988e-05,  3.4786e-05,  4.6001e-05,\n",
      "          5.7285e-05,  6.9091e-05,  8.0653e-05,  9.2228e-05, -7.7095e-05,\n",
      "         -1.5413e-04, -2.3124e-04, -3.0828e-04, -3.8516e-04, -4.6249e-04,\n",
      "         -5.3936e-04, -6.1693e-04, -1.5815e-04, -3.1613e-04, -4.7464e-04,\n",
      "         -6.3246e-04, -7.9133e-04, -9.4912e-04, -1.1077e-03, -1.2659e-03,\n",
      "          6.0543e-05,  1.2106e-04,  1.8146e-04,  2.4189e-04,  3.0221e-04,\n",
      "          3.6338e-04,  4.2421e-04,  4.8449e-04, -2.1349e-05, -4.3073e-05,\n",
      "         -6.4198e-05, -8.6457e-05, -1.0718e-04, -1.2774e-04, -1.5152e-04,\n",
      "         -1.7114e-04,  5.1069e-06,  1.0243e-05,  1.4793e-05,  1.9966e-05,\n",
      "          2.5591e-05,  2.9368e-05,  3.4965e-05,  4.0624e-05,  6.0240e-06,\n",
      "          1.2129e-05,  1.8081e-05,  2.4113e-05,  2.9773e-05,  3.6093e-05,\n",
      "          4.2003e-05,  4.8772e-05, -9.4963e-05, -1.8990e-04, -2.8491e-04,\n",
      "         -3.7982e-04, -4.7475e-04, -5.6971e-04, -6.6497e-04, -7.5905e-04,\n",
      "          3.3585e-05, -1.2469e-05,  1.1534e-05, -7.7058e-05, -1.5817e-04,\n",
      "          6.0672e-05, -2.1306e-05,  5.0130e-06,  6.0636e-06, -9.4942e-05,\n",
      "         -9.9490e-04,  3.4024e-04, -3.0293e-04,  2.2915e-03,  4.7090e-03,\n",
      "         -1.7865e-03,  5.9086e-04, -1.1541e-04, -1.6007e-04,  2.8369e-03,\n",
      "         -1.8020e-03,  8.7276e-04, -8.0258e-04,  4.1162e-03,  8.4288e-03,\n",
      "         -3.2975e-03,  1.3641e-03, -4.2616e-04, -4.1377e-04,  5.0024e-03,\n",
      "         -7.0181e-04,  8.9492e-05, -6.4736e-05,  1.6464e-03,  3.4042e-03,\n",
      "         -1.2315e-03,  2.4434e-04,  4.9592e-05, -4.0582e-05,  2.0910e-03,\n",
      "         -7.6145e-04,  3.6126e-04, -3.3402e-04,  1.7387e-03,  3.5622e-03,\n",
      "         -1.3901e-03,  5.6734e-04, -1.7535e-04, -1.7156e-04,  2.1146e-03,\n",
      "          7.4763e-04, -4.7746e-04,  4.4930e-04, -1.6875e-03, -3.4363e-03,\n",
      "          1.3917e-03, -6.9950e-04,  2.7979e-04,  2.2802e-04, -2.0081e-03,\n",
      "         -7.7206e-04,  1.1825e-04, -8.8345e-05,  1.8008e-03,  3.7201e-03,\n",
      "         -1.3528e-03,  2.8912e-04,  3.7182e-05, -5.2944e-05,  2.2838e-03,\n",
      "          1.1241e-03, -6.2191e-04,  5.8195e-04, -2.5530e-03, -5.2134e-03,\n",
      "          2.0723e-03, -9.4262e-04,  3.3728e-04,  2.9715e-04, -3.0760e-03,\n",
      "         -1.1510e-03,  4.4817e-04, -4.0485e-04,  2.6431e-03,  5.4258e-03,\n",
      "         -2.0794e-03,  7.4575e-04, -1.8026e-04, -2.1163e-04,  3.2525e-03,\n",
      "         -1.7556e-03,  5.5019e-04, -4.8464e-04,  4.0493e-03,  8.3280e-03,\n",
      "         -3.1403e-03,  9.8390e-04, -1.5996e-04, -2.5836e-04,  5.0326e-03,\n",
      "         -1.3672e-03,  4.9325e-04, -4.4159e-04,  3.1481e-03,  6.4670e-03,\n",
      "         -2.4627e-03,  8.4098e-04, -1.8008e-04, -2.3248e-04,  3.8882e-03,\n",
      "         -1.7559e-03,  8.2971e-03, -8.0580e-03,  3.2155e-03, -8.7178e-03,\n",
      "         -8.0002e-03, -8.8348e-03,  4.2489e-04, -5.3378e-03, -1.2761e-03,\n",
      "         -3.3278e-03,  1.0174e-02, -1.1975e-02,  3.8893e-03, -1.1668e-02,\n",
      "         -1.1952e-02, -1.1511e-02, -3.8250e-04, -9.0319e-03, -2.9690e-03,\n",
      "         -3.0511e-01]])\n"
     ]
    }
   ],
   "source": [
    "λ = 0.1\n",
    "r = torch.matmul((-torch.inverse(h+λ*torch.eye(211))),torch.transpose(torch.unsqueeze(grad_f, 0),0, 1))\n",
    "print(r)\n",
    "print(torch.transpose(r,0,1))\n",
    "#-torch.inverse(h+λ*torch.eye(211))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.2990e-05],\n",
      "        [ 6.7240e-05],\n",
      "        [ 1.0042e-04],\n",
      "        [ 1.3441e-04],\n",
      "        [ 1.6783e-04],\n",
      "        [ 2.0080e-04],\n",
      "        [ 2.3457e-04],\n",
      "        [ 2.6874e-04],\n",
      "        [-1.2525e-05],\n",
      "        [-2.5565e-05],\n",
      "        [-3.9203e-05],\n",
      "        [-5.1367e-05],\n",
      "        [-6.4396e-05],\n",
      "        [-7.6674e-05],\n",
      "        [-8.8923e-05],\n",
      "        [-1.0275e-04],\n",
      "        [ 1.1452e-05],\n",
      "        [ 2.2988e-05],\n",
      "        [ 3.4786e-05],\n",
      "        [ 4.6001e-05],\n",
      "        [ 5.7285e-05],\n",
      "        [ 6.9091e-05],\n",
      "        [ 8.0653e-05],\n",
      "        [ 9.2228e-05],\n",
      "        [-7.7095e-05],\n",
      "        [-1.5413e-04],\n",
      "        [-2.3124e-04],\n",
      "        [-3.0828e-04],\n",
      "        [-3.8516e-04],\n",
      "        [-4.6249e-04],\n",
      "        [-5.3936e-04],\n",
      "        [-6.1693e-04],\n",
      "        [-1.5815e-04],\n",
      "        [-3.1613e-04],\n",
      "        [-4.7464e-04],\n",
      "        [-6.3246e-04],\n",
      "        [-7.9133e-04],\n",
      "        [-9.4912e-04],\n",
      "        [-1.1077e-03],\n",
      "        [-1.2659e-03],\n",
      "        [ 6.0543e-05],\n",
      "        [ 1.2106e-04],\n",
      "        [ 1.8146e-04],\n",
      "        [ 2.4189e-04],\n",
      "        [ 3.0221e-04],\n",
      "        [ 3.6338e-04],\n",
      "        [ 4.2421e-04],\n",
      "        [ 4.8449e-04],\n",
      "        [-2.1349e-05],\n",
      "        [-4.3073e-05],\n",
      "        [-6.4198e-05],\n",
      "        [-8.6457e-05],\n",
      "        [-1.0718e-04],\n",
      "        [-1.2774e-04],\n",
      "        [-1.5152e-04],\n",
      "        [-1.7114e-04],\n",
      "        [ 5.1069e-06],\n",
      "        [ 1.0243e-05],\n",
      "        [ 1.4793e-05],\n",
      "        [ 1.9966e-05],\n",
      "        [ 2.5591e-05],\n",
      "        [ 2.9368e-05],\n",
      "        [ 3.4965e-05],\n",
      "        [ 4.0624e-05],\n",
      "        [ 6.0240e-06],\n",
      "        [ 1.2129e-05],\n",
      "        [ 1.8081e-05],\n",
      "        [ 2.4113e-05],\n",
      "        [ 2.9773e-05],\n",
      "        [ 3.6093e-05],\n",
      "        [ 4.2003e-05],\n",
      "        [ 4.8772e-05],\n",
      "        [-9.4963e-05],\n",
      "        [-1.8990e-04],\n",
      "        [-2.8491e-04],\n",
      "        [-3.7982e-04],\n",
      "        [-4.7475e-04],\n",
      "        [-5.6971e-04],\n",
      "        [-6.6497e-04],\n",
      "        [-7.5905e-04],\n",
      "        [ 3.3585e-05],\n",
      "        [-1.2469e-05],\n",
      "        [ 1.1534e-05],\n",
      "        [-7.7058e-05],\n",
      "        [-1.5817e-04],\n",
      "        [ 6.0672e-05],\n",
      "        [-2.1306e-05],\n",
      "        [ 5.0130e-06],\n",
      "        [ 6.0636e-06],\n",
      "        [-9.4942e-05],\n",
      "        [-9.9490e-04],\n",
      "        [ 3.4024e-04],\n",
      "        [-3.0293e-04],\n",
      "        [ 2.2915e-03],\n",
      "        [ 4.7090e-03],\n",
      "        [-1.7865e-03],\n",
      "        [ 5.9086e-04],\n",
      "        [-1.1541e-04],\n",
      "        [-1.6007e-04],\n",
      "        [ 2.8369e-03],\n",
      "        [-1.8020e-03],\n",
      "        [ 8.7276e-04],\n",
      "        [-8.0258e-04],\n",
      "        [ 4.1162e-03],\n",
      "        [ 8.4288e-03],\n",
      "        [-3.2975e-03],\n",
      "        [ 1.3641e-03],\n",
      "        [-4.2616e-04],\n",
      "        [-4.1377e-04],\n",
      "        [ 5.0024e-03],\n",
      "        [-7.0181e-04],\n",
      "        [ 8.9492e-05],\n",
      "        [-6.4736e-05],\n",
      "        [ 1.6464e-03],\n",
      "        [ 3.4042e-03],\n",
      "        [-1.2315e-03],\n",
      "        [ 2.4434e-04],\n",
      "        [ 4.9592e-05],\n",
      "        [-4.0582e-05],\n",
      "        [ 2.0910e-03],\n",
      "        [-7.6145e-04],\n",
      "        [ 3.6126e-04],\n",
      "        [-3.3402e-04],\n",
      "        [ 1.7387e-03],\n",
      "        [ 3.5622e-03],\n",
      "        [-1.3901e-03],\n",
      "        [ 5.6734e-04],\n",
      "        [-1.7535e-04],\n",
      "        [-1.7156e-04],\n",
      "        [ 2.1146e-03],\n",
      "        [ 7.4763e-04],\n",
      "        [-4.7746e-04],\n",
      "        [ 4.4930e-04],\n",
      "        [-1.6875e-03],\n",
      "        [-3.4363e-03],\n",
      "        [ 1.3917e-03],\n",
      "        [-6.9950e-04],\n",
      "        [ 2.7979e-04],\n",
      "        [ 2.2802e-04],\n",
      "        [-2.0081e-03],\n",
      "        [-7.7206e-04],\n",
      "        [ 1.1825e-04],\n",
      "        [-8.8345e-05],\n",
      "        [ 1.8008e-03],\n",
      "        [ 3.7201e-03],\n",
      "        [-1.3528e-03],\n",
      "        [ 2.8912e-04],\n",
      "        [ 3.7182e-05],\n",
      "        [-5.2944e-05],\n",
      "        [ 2.2838e-03],\n",
      "        [ 1.1241e-03],\n",
      "        [-6.2191e-04],\n",
      "        [ 5.8195e-04],\n",
      "        [-2.5530e-03],\n",
      "        [-5.2134e-03],\n",
      "        [ 2.0723e-03],\n",
      "        [-9.4262e-04],\n",
      "        [ 3.3728e-04],\n",
      "        [ 2.9715e-04],\n",
      "        [-3.0760e-03],\n",
      "        [-1.1510e-03],\n",
      "        [ 4.4817e-04],\n",
      "        [-4.0485e-04],\n",
      "        [ 2.6431e-03],\n",
      "        [ 5.4258e-03],\n",
      "        [-2.0794e-03],\n",
      "        [ 7.4575e-04],\n",
      "        [-1.8026e-04],\n",
      "        [-2.1163e-04],\n",
      "        [ 3.2525e-03],\n",
      "        [-1.7556e-03],\n",
      "        [ 5.5019e-04],\n",
      "        [-4.8464e-04],\n",
      "        [ 4.0493e-03],\n",
      "        [ 8.3280e-03],\n",
      "        [-3.1403e-03],\n",
      "        [ 9.8390e-04],\n",
      "        [-1.5996e-04],\n",
      "        [-2.5836e-04],\n",
      "        [ 5.0326e-03],\n",
      "        [-1.3672e-03],\n",
      "        [ 4.9325e-04],\n",
      "        [-4.4159e-04],\n",
      "        [ 3.1481e-03],\n",
      "        [ 6.4670e-03],\n",
      "        [-2.4627e-03],\n",
      "        [ 8.4098e-04],\n",
      "        [-1.8008e-04],\n",
      "        [-2.3248e-04],\n",
      "        [ 3.8882e-03],\n",
      "        [-1.7559e-03],\n",
      "        [ 8.2971e-03],\n",
      "        [-8.0580e-03],\n",
      "        [ 3.2155e-03],\n",
      "        [-8.7178e-03],\n",
      "        [-8.0002e-03],\n",
      "        [-8.8348e-03],\n",
      "        [ 4.2489e-04],\n",
      "        [-5.3378e-03],\n",
      "        [-1.2761e-03],\n",
      "        [-3.3278e-03],\n",
      "        [ 1.0174e-02],\n",
      "        [-1.1975e-02],\n",
      "        [ 3.8893e-03],\n",
      "        [-1.1668e-02],\n",
      "        [-1.1952e-02],\n",
      "        [-1.1511e-02],\n",
      "        [-3.8250e-04],\n",
      "        [-9.0319e-03],\n",
      "        [-2.9690e-03],\n",
      "        [-3.0511e-01]])\n"
     ]
    }
   ],
   "source": [
    "print(r)\n",
    "#print(torch.transpose(torch.unsqueeze(grad_f, 0),0, 1).shape)\n",
    "\n",
    "#print(-(0.1*torch.eye(211)+torch.eye(211)))\n",
    "#print([_.view(-1) for _ in red.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1163, -1.0359,  0.8326,  0.0294, -0.3389, -1.1610, -0.2600,  1.3225,\n",
      "          0.2151,  0.6913,  1.1016,  0.3517, -1.0926,  0.1376,  0.2224, -0.6764,\n",
      "         -0.3170,  0.9312, -1.2252,  0.5269,  1.4364, -0.1911, -0.0458, -0.0723,\n",
      "          1.2971,  1.0933, -0.8288,  1.0118,  0.1136, -1.4217, -1.6867,  1.3411,\n",
      "         -0.2619, -0.8584,  0.4098,  1.6572,  0.7964, -0.5192, -1.0067, -0.4939,\n",
      "          0.1677, -0.3507, -0.4070, -1.2863,  0.5181, -0.8040,  0.0899,  0.5161,\n",
      "         -0.8158, -0.2558, -0.0743,  2.2494,  1.4208, -1.4235,  1.3553, -0.0055,\n",
      "          0.7172,  0.1068,  0.2349,  0.8284,  1.4275,  0.2545,  0.3010,  0.8738,\n",
      "          1.3722,  0.8892,  0.4251, -0.7980,  0.4930, -2.1455, -0.6002,  0.2468,\n",
      "          0.2561, -0.2038, -1.0521, -0.2180,  0.3193, -1.3633,  0.1726,  0.2693]])\n",
      "tensor([[ 0.1163, -1.0359,  0.8326,  0.0294, -0.3389, -1.1610, -0.2600,  1.3225],\n",
      "        [ 0.2151,  0.6913,  1.1016,  0.3517, -1.0926,  0.1376,  0.2224, -0.6764],\n",
      "        [-0.3170,  0.9312, -1.2252,  0.5269,  1.4364, -0.1911, -0.0458, -0.0723],\n",
      "        [ 1.2971,  1.0933, -0.8288,  1.0118,  0.1136, -1.4217, -1.6867,  1.3411],\n",
      "        [-0.2619, -0.8584,  0.4098,  1.6572,  0.7964, -0.5192, -1.0067, -0.4939],\n",
      "        [ 0.1677, -0.3507, -0.4070, -1.2863,  0.5181, -0.8040,  0.0899,  0.5161],\n",
      "        [-0.8158, -0.2558, -0.0743,  2.2494,  1.4208, -1.4235,  1.3553, -0.0055],\n",
      "        [ 0.7172,  0.1068,  0.2349,  0.8284,  1.4275,  0.2545,  0.3010,  0.8738],\n",
      "        [ 1.3722,  0.8892,  0.4251, -0.7980,  0.4930, -2.1455, -0.6002,  0.2468],\n",
      "        [ 0.2561, -0.2038, -1.0521, -0.2180,  0.3193, -1.3633,  0.1726,  0.2693]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(1,80)\n",
    "print(x)\n",
    "y = []\n",
    "for i in red.parameters():\n",
    "    y.append(i.shape)\n",
    "#x = x.view(y[0])\n",
    "print(x.view(y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor existente:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Tensor ajustado con la misma forma que el otro tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tienes dos tensores con diferentes formas\n",
    "tensor_existente = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "otro_tensor = torch.tensor([[7, 8], [9, 10],[11,12]])\n",
    "\n",
    "# Obtén la forma del otro tensor\n",
    "forma_deseada = otro_tensor.shape\n",
    "\n",
    "# Ajusta la forma del tensor existente para que coincida con la forma deseada\n",
    "tensor_ajustado = tensor_existente.view(forma_deseada)\n",
    "\n",
    "# Alternativamente, puedes usar reshape\n",
    "# tensor_ajustado = tensor_existente.reshape(forma_deseada)\n",
    "\n",
    "print(\"Tensor existente:\")\n",
    "print(tensor_existente)\n",
    "\n",
    "print(\"Tensor ajustado con la misma forma que el otro tensor:\")\n",
    "print(tensor_ajustado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [1., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[12.,  0.],\n",
       "          [ 0.,  0.]],\n",
       "\n",
       "         [[ 0., 24.],\n",
       "          [ 0.,  0.]]],\n",
       "\n",
       "\n",
       "        [[[ 0.,  0.],\n",
       "          [ 6.,  0.]],\n",
       "\n",
       "         [[ 0.,  0.],\n",
       "          [ 0., 18.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pow_reducer(x):\n",
    "    x=x.view(-1)\n",
    "    return x.pow(3).sum()\n",
    "inputs = torch.Tensor([[2,4],[1,3]])\n",
    "print(inputs)\n",
    "torch.autograd.functional.hessian(pow_reducer, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [1., 3.]])\n",
      "tensor([2., 4., 1., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor([[2,4],[1,3]]))\n",
    "print(torch.Tensor([[2,4],[1,3]]).view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "def pow_r(x):\n",
    "    return x.pow(3).sum()\n",
    "\n",
    "print(pow_r(torch.Tensor([2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([288])\n",
      "torch.Size([32, 1, 3, 3])\n",
      "tensor([[[[-0.1983, -1.9691,  0.0434,  ..., -1.3908,  0.4771, -0.3967],\n",
      "          [-1.1117,  1.4040, -0.6951,  ...,  0.6863, -0.0186,  0.9661],\n",
      "          [-0.1227,  0.9660,  0.9239,  ..., -0.7129, -1.0009,  1.2682],\n",
      "          ...,\n",
      "          [-0.2937, -1.8352,  0.6319,  ...,  0.6301,  1.0036,  0.1321],\n",
      "          [-0.1069,  0.1753,  1.9536,  ..., -1.3419, -2.1442,  0.1459],\n",
      "          [-0.9335, -0.2808, -0.4531,  ...,  1.2536, -1.7289,  0.6452]]],\n",
      "\n",
      "\n",
      "        [[[-1.2618, -0.5357,  0.1832,  ..., -1.1228, -0.5770, -2.1551],\n",
      "          [-0.5711, -0.0169,  0.3976,  ..., -0.2121, -0.0506,  0.4513],\n",
      "          [ 1.3490,  2.0045, -0.8415,  ..., -0.4678, -0.1075, -0.7057],\n",
      "          ...,\n",
      "          [ 0.6545,  1.0627,  0.6962,  ...,  0.1561, -1.2382,  0.4118],\n",
      "          [ 0.3219,  0.9007,  1.0146,  ..., -0.7255, -0.7326, -0.4443],\n",
      "          [ 1.1722, -1.3391, -1.1455,  ..., -0.7805, -0.1412, -0.6500]]],\n",
      "\n",
      "\n",
      "        [[[-0.3720,  3.1226, -1.0307,  ..., -1.2306, -0.7086, -1.5233],\n",
      "          [ 0.3987,  0.6742, -0.0702,  ..., -1.2933, -0.5488,  0.7355],\n",
      "          [ 2.2543,  1.8145, -1.5769,  ..., -1.6488, -0.6532, -0.6913],\n",
      "          ...,\n",
      "          [ 0.0716,  0.0159, -2.6818,  ...,  0.4660,  0.2517,  0.6994],\n",
      "          [ 0.3108,  0.8736,  0.0062,  ...,  0.3141, -2.1490,  0.8307],\n",
      "          [-0.4401,  0.5236,  0.6983,  ..., -0.9086, -0.5833,  0.2766]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5155,  2.7530, -1.5299,  ..., -0.0889, -1.0527, -1.4829],\n",
      "          [ 2.0835, -0.7751, -0.4668,  ...,  0.9465,  0.9661, -2.6791],\n",
      "          [ 0.4461,  1.2836, -1.7894,  ..., -0.0589, -0.2172,  1.5025],\n",
      "          ...,\n",
      "          [ 0.1280,  0.3778,  0.9147,  ..., -1.6636, -0.3356,  0.0344],\n",
      "          [-0.9381, -0.1623,  0.2066,  ..., -1.1359,  1.2285,  0.4577],\n",
      "          [ 0.6606,  0.0981,  0.3484,  ..., -1.0198,  1.1668,  0.9353]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2659, -1.8722,  1.8072,  ..., -0.9618, -1.3272, -0.5464],\n",
      "          [ 0.9976,  0.7332,  0.2240,  ...,  1.0627, -0.3255,  0.5365],\n",
      "          [ 1.1683,  0.2829,  2.1175,  ..., -0.7243, -0.0992, -0.9524],\n",
      "          ...,\n",
      "          [-0.3171, -0.1350, -0.5214,  ...,  0.1089,  0.5143, -1.7933],\n",
      "          [ 0.1408,  0.5765,  1.0261,  ...,  1.3760, -2.5232,  1.1367],\n",
      "          [-0.0582,  0.6755, -0.1491,  ..., -1.4623, -0.3428, -1.4384]]],\n",
      "\n",
      "\n",
      "        [[[-0.8757,  1.1234,  0.0563,  ..., -0.2450, -0.4039,  1.1374],\n",
      "          [ 0.2794,  0.4265,  2.3651,  ...,  0.1541, -1.5086, -0.5466],\n",
      "          [ 0.7078, -0.7022,  0.9251,  ...,  0.4149,  1.3865,  1.1475],\n",
      "          ...,\n",
      "          [ 0.8731, -0.3453,  0.1027,  ...,  0.7350,  1.8375, -0.3342],\n",
      "          [ 0.0649,  0.3565, -0.5332,  ...,  1.8315, -0.6318, -1.3242],\n",
      "          [ 1.0160,  0.6027,  0.2251,  ..., -0.1397,  0.4417, -0.9680]]]])\n"
     ]
    }
   ],
   "source": [
    "net = Net(12, 12)\n",
    "\n",
    "print([i.view(-1) for i in net.parameters()][0].size())\n",
    "print([i for i in net.parameters()][0].size())\n",
    "\n",
    "t = tuple([_.view(-1) for _ in net.parameters()])\n",
    "\n",
    "print(torch.randn(size=[8, 1, 12, 12], dtype=torch.float32))\n",
    "#print(net)\n",
    "#print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0822e-01, -6.4565e-02, -1.7312e-01, -1.4084e-01,  2.7732e-04,\n",
       "          3.2908e-01,  3.0664e-01,  2.0747e-01, -2.5428e-01, -3.2957e-01,\n",
       "         -1.9952e-01, -1.9029e-01,  2.3390e-01, -2.7291e-01, -2.5889e-01,\n",
       "          1.4640e-01,  1.2072e-01,  1.6149e-01,  2.7986e-01,  2.8060e-01,\n",
       "          3.1479e-01, -3.2459e-01,  1.6795e-01, -1.9119e-01, -6.2354e-02,\n",
       "         -6.2312e-02,  2.0500e-01,  2.3773e-01, -1.2552e-01,  3.2771e-01,\n",
       "          1.8885e-01, -2.4989e-01,  2.6277e-01, -2.9609e-01, -3.4744e-02,\n",
       "          1.0740e-01, -2.9994e-02,  1.5956e-01,  7.0270e-02,  2.6984e-01,\n",
       "          4.3494e-02,  1.4707e-01,  2.1189e-01, -7.7057e-02,  2.3074e-01,\n",
       "          1.1717e-01,  2.6989e-01, -7.5095e-02, -1.2888e-01, -2.1737e-01,\n",
       "         -2.7852e-01, -1.2524e-01, -2.8630e-01, -2.1395e-01,  2.0479e-01,\n",
       "          1.1009e-01,  1.6142e-01,  2.8751e-01,  3.2095e-01, -1.1432e-01,\n",
       "         -6.7380e-02,  1.4659e-01, -2.0638e-01,  1.4246e-02, -4.1830e-02,\n",
       "          5.3237e-02, -1.9791e-01, -3.1044e-01, -2.9105e-01, -1.4958e-01,\n",
       "         -1.8506e-01, -5.3857e-03, -2.8689e-01,  1.1233e-03,  2.9373e-01,\n",
       "          2.3318e-02,  1.7893e-02,  9.8911e-02,  6.3532e-02, -2.8384e-01,\n",
       "         -1.1619e-01, -3.0702e-01,  3.0836e-01, -3.1545e-01,  1.0914e-01,\n",
       "          3.2633e-01, -7.0716e-02, -1.9645e-01, -1.1160e-01,  3.7116e-02,\n",
       "         -1.0937e-01,  2.4583e-01,  2.2461e-01, -6.1997e-02, -2.0775e-01,\n",
       "         -3.1602e-01,  3.2919e-01, -2.2508e-01, -2.5597e-01,  2.6382e-01,\n",
       "         -1.0528e-01,  1.2964e-01,  3.0498e-01, -9.8968e-02,  1.6432e-01,\n",
       "          2.9822e-01, -2.2602e-02,  2.4436e-01, -2.1235e-01,  1.8949e-01,\n",
       "          2.7801e-01, -1.5143e-02,  2.0055e-01, -2.9444e-01, -1.9481e-01,\n",
       "         -6.1485e-02, -1.0983e-01, -3.3300e-01, -1.4427e-01,  3.2377e-01,\n",
       "          2.6335e-01,  2.2092e-01,  1.2893e-01, -1.2199e-02,  2.9854e-01,\n",
       "          5.0728e-02,  1.6881e-01, -2.4920e-01,  1.8482e-02,  2.3996e-01,\n",
       "         -5.8027e-02,  2.9791e-01,  2.0746e-01, -5.9308e-02,  7.9312e-02,\n",
       "         -1.9623e-02, -1.0193e-01,  3.5430e-02,  1.2687e-01, -1.6602e-02,\n",
       "         -1.7864e-01, -1.4775e-01, -2.1079e-01,  2.3189e-01, -2.7094e-01,\n",
       "         -1.8041e-01, -1.1173e-01, -2.7765e-01, -6.7510e-03,  1.6758e-01,\n",
       "          2.5755e-01, -1.5549e-01, -7.8368e-02, -8.6754e-02,  1.0586e-01,\n",
       "          1.3529e-02,  3.0381e-01, -1.1117e-01, -9.3800e-02, -3.2934e-01,\n",
       "         -1.8997e-01,  1.2196e-01, -1.7749e-01,  2.3600e-01, -1.8730e-02,\n",
       "         -1.3999e-01, -1.8610e-01, -2.6775e-01, -3.2316e-01, -1.4838e-01,\n",
       "         -6.4402e-02, -2.5104e-01, -2.4346e-01,  3.1004e-01, -2.1248e-01,\n",
       "         -1.7718e-01, -1.7630e-01,  1.4518e-01, -2.2953e-01,  2.2339e-01,\n",
       "         -4.3224e-02, -3.2425e-01,  9.4129e-02,  5.8462e-02,  1.1265e-01,\n",
       "          3.3102e-01, -1.9686e-02, -2.9274e-01, -1.1827e-01,  2.5710e-01,\n",
       "         -1.1282e-01, -2.0725e-01, -3.3037e-01,  2.1381e-01, -2.6244e-01,\n",
       "         -1.1096e-01,  3.2028e-01, -7.2844e-02, -2.9237e-01,  1.9178e-01,\n",
       "          2.2150e-01,  7.9685e-02,  3.2150e-01, -2.2568e-01, -3.7515e-02,\n",
       "         -1.1201e-01,  1.7716e-01, -3.0695e-01, -2.2827e-01,  1.5041e-01,\n",
       "          1.7473e-01,  1.8526e-01, -2.8719e-01,  2.1631e-02, -9.7792e-03,\n",
       "         -1.8700e-01,  5.5315e-02,  4.5613e-04,  6.1076e-02,  3.1423e-01,\n",
       "         -1.1496e-01, -2.8186e-01, -1.3982e-01, -1.9174e-01,  1.3298e-01,\n",
       "         -1.4812e-01, -1.3587e-01,  9.1315e-02, -1.9749e-01,  1.3240e-01,\n",
       "         -2.8188e-01,  4.4060e-02,  6.0227e-02,  1.3420e-01,  2.9579e-01,\n",
       "         -3.2340e-01, -1.8974e-01, -2.6819e-01, -1.5551e-02, -3.0380e-01,\n",
       "         -3.0264e-01,  1.0531e-01,  2.1241e-02,  2.4822e-01, -4.6497e-02,\n",
       "         -2.1974e-01,  2.5950e-01,  1.5662e-01, -1.5745e-01,  1.0142e-01,\n",
       "         -3.0534e-01,  4.3300e-02,  2.9368e-01,  2.2107e-02,  1.3136e-01,\n",
       "          1.9721e-01,  2.0347e-01,  9.5081e-02, -2.7311e-01,  1.1074e-02,\n",
       "         -1.4590e-01, -1.2755e-01, -3.2451e-01, -3.2685e-01,  2.5906e-01,\n",
       "          5.3853e-02,  2.7590e-01,  9.0288e-02, -1.0979e-01, -3.0428e-01,\n",
       "          1.0632e-01, -2.7588e-01, -1.1040e-02, -2.7866e-01,  9.0997e-02,\n",
       "         -1.7049e-01, -6.5986e-02,  3.2783e-01, -5.4513e-02,  1.3126e-02,\n",
       "         -1.5311e-01, -2.8603e-01,  2.9589e-01, -8.2979e-02,  3.0681e-01,\n",
       "          1.7149e-01, -1.7208e-01,  2.4458e-01], grad_fn=<ViewBackward0>),\n",
       " tensor([-0.1023, -0.0036,  0.0535,  0.1899,  0.1758,  0.2878, -0.1456, -0.1188,\n",
       "          0.1146,  0.0542, -0.0272,  0.1749, -0.2523, -0.3180, -0.0937, -0.1898,\n",
       "          0.0432,  0.0039,  0.2433, -0.1232,  0.1588,  0.2373, -0.0286, -0.1221,\n",
       "          0.2550,  0.3042,  0.2810,  0.0937, -0.2614, -0.1366, -0.1231,  0.1835],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([ 0.0013, -0.0146,  0.0137,  ...,  0.0110, -0.0118,  0.0053],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([ 0.0091, -0.0016, -0.0132, -0.0115, -0.0064], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()]))\n",
    "tuple([_.view(-1) for _ in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6262/1183573972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# v = torch.cat([_.view(-1) for _ in red.parameters()], dim = 0)#concatena los paremetros de la red en un solo vector unidimensional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6262/1183573972.py\u001b[0m in \u001b[0;36mexec\u001b[0;34m(self, epocas)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepocas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mf_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcula_perdida\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_convierte_parametros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepocas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"for {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6262/1183573972.py\u001b[0m in \u001b[0;36mcalcula_perdida\u001b[0;34m(self, *parametros)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalcula_perdida\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparametros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mn_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masigna_parametros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparametros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#Recrea el funcionamiento de la red\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6262/1183573972.py\u001b[0m in \u001b[0;36masigna_parametros\u001b[0;34m(self, reasignar, *parametros)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mn_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mr_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mp_partida\u001b[0m \u001b[0;31m#se obtiene la dimension del primer conjunto de parametros de la red\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mn_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_partida\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreasignar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "import levenberg_marquardt as lm\n",
    "\n",
    "red = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "#input = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "\n",
    "# print(input)\n",
    "entrada = input #la entrada se da como un parametro global\n",
    "salida_esperada = torch.tensor([-0.0834]) #lo mismo para la salida esperada\n",
    "# λ = 0.1\n",
    "\n",
    "lm = LM(red)\n",
    "lm.exec()\n",
    "\n",
    "# v = torch.cat([_.view(-1) for _ in red.parameters()], dim = 0)#concatena los paremetros de la red en un solo vector unidimensional\n",
    "# h = torch.autograd.functional.hessian(lm.calcula_perdida, v) #calculamos la matriz hessiana\n",
    "# grad_f = torch.autograd.grad(lm.calcula_perdida(v), v)[0] #calculamos el gradiente de la funcion\n",
    "# r = -torch.inverse(h+λ*torch.eye(211))*torch.transpose(grad_f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
