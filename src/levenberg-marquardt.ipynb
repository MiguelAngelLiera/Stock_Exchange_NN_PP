{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from NARNN import NARNN\n",
    "\n",
    "from levenberg_marquardt import LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0457], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/miguel/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "red = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "#input = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "\n",
    "# print(input)\n",
    "entrada = input #la entrada se da como un parametro global\n",
    "salida_esperada = torch.tensor([-0.0834]) #lo mismo para la salida esperada\n",
    "# λ = 0.1\n",
    "\n",
    "\n",
    "\n",
    "print(red(torch.Tensor([1,2,3,4,5,6,7,8])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes\n",
      "[Parameter containing:\n",
      "tensor([[-0.0797,  0.0073, -0.0598,  0.1786, -0.2524,  0.0537,  0.2277, -0.1192],\n",
      "        [-0.1985,  0.2473,  0.0691, -0.0271,  0.1058,  0.0982, -0.1964, -0.1613],\n",
      "        [ 0.0789,  0.1483, -0.1321, -0.0894,  0.2940, -0.2954,  0.1811,  0.2621],\n",
      "        [-0.2028,  0.2626, -0.1991, -0.2144,  0.2224,  0.2139,  0.2694,  0.1281],\n",
      "        [ 0.0284, -0.0204,  0.1192,  0.2104, -0.0191, -0.0847, -0.3487, -0.2058],\n",
      "        [ 0.0412,  0.2255,  0.2777, -0.3318, -0.2635, -0.2094, -0.3100, -0.2829],\n",
      "        [-0.3337, -0.2234,  0.1577,  0.3484, -0.1335, -0.0568, -0.1739,  0.2567],\n",
      "        [ 0.1404, -0.0089, -0.1057, -0.1257, -0.2342, -0.0408,  0.3042, -0.0711],\n",
      "        [-0.1237, -0.0345, -0.2599,  0.1833,  0.1387, -0.2969, -0.0318, -0.2513],\n",
      "        [-0.1788,  0.1941,  0.3322,  0.3085, -0.2480, -0.0149,  0.1150, -0.0085]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2034,  0.2809, -0.0021, -0.1663, -0.3272,  0.1422,  0.2390, -0.1130,\n",
      "        -0.1114, -0.1568], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0716,  0.0703, -0.2236, -0.1895,  0.0386, -0.0282,  0.1429,  0.0309,\n",
      "         -0.2954,  0.3079],\n",
      "        [ 0.2493,  0.1758,  0.0965,  0.1749,  0.1460, -0.1735,  0.2040,  0.0921,\n",
      "         -0.2512, -0.0713],\n",
      "        [-0.0357,  0.2083,  0.1396,  0.1112, -0.0850, -0.2878, -0.2793, -0.0076,\n",
      "          0.1927,  0.2354],\n",
      "        [-0.2358, -0.0821,  0.1432, -0.2937,  0.0863,  0.0754,  0.2660, -0.1437,\n",
      "         -0.1203,  0.0661],\n",
      "        [ 0.1313,  0.1036,  0.1514,  0.1806, -0.2668, -0.0304,  0.2890,  0.0875,\n",
      "         -0.0655, -0.1172],\n",
      "        [ 0.0705, -0.1363, -0.1588,  0.1765,  0.1665, -0.2729,  0.2792, -0.2846,\n",
      "          0.0646, -0.2204],\n",
      "        [-0.1273,  0.0962,  0.2349,  0.1345,  0.2388, -0.2643,  0.1018, -0.0218,\n",
      "          0.1851,  0.2848],\n",
      "        [ 0.1369, -0.2609,  0.0820,  0.1083, -0.0295,  0.1991, -0.1900, -0.0313,\n",
      "         -0.2735, -0.2113],\n",
      "        [ 0.2790,  0.2301, -0.1287, -0.0031,  0.1239, -0.1574,  0.3025, -0.0104,\n",
      "         -0.1040, -0.1227],\n",
      "        [-0.2860,  0.1324, -0.2814, -0.2821,  0.0304, -0.2050, -0.0514,  0.2339,\n",
      "          0.2710, -0.2950]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2268, -0.2472,  0.0008, -0.3115, -0.1629,  0.0887,  0.1102,  0.1602,\n",
      "        -0.0810, -0.2564], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2365, -0.0298,  0.1491, -0.0752, -0.2793,  0.2249, -0.1013, -0.2184,\n",
      "         -0.1650, -0.1256]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2164], requires_grad=True)]\n",
      "tensor([0.4884], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Antes\")\n",
    "print([i for i in red.parameters()])\n",
    "print(red(torch.Tensor([1,2,3,4,5,6,7,8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aux_convierte_parametros\n",
      "asigna parametros\n",
      "aux_convierte_parametros\n",
      "asigna parametros\n",
      "asigna parametros\n",
      "asigna parametros\n",
      "aux_convierte_parametros\n",
      "asigna parametros\n",
      "ant: tensor(0.1901, grad_fn=<MseLossBackward0>)\n",
      "nuevo: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "aux_convierte_parametros\n",
      "asigna parametros\n",
      "asigna parametros\n",
      "asigna parametros\n",
      "aux_convierte_parametros\n",
      "asigna parametros\n",
      "ant: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "nuevo: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "aux_convierte_parametros\n",
      "asigna parametros\n",
      "asigna parametros\n",
      "asigna parametros\n",
      "aux_convierte_parametros\n",
      "asigna parametros\n",
      "ant: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "nuevo: tensor(6.4258e-08, grad_fn=<MseLossBackward0>)\n",
      "Despues\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(red(torch.Tensor([1,2,3,4,5,6,7,8])))\n",
    "lm = LM(red, torch.Tensor([0.0574, 0.0526, 0.0532, 0.0526, 0.0574, 0.0446, 0.0259, 0.0728]),torch.tensor([0.0675]))\n",
    "lm.exec()\n",
    "print(\"Despues\")\n",
    "#print(red(torch.Tensor([1,2,3,4,5,6,7,8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despues\n",
      "[Parameter containing:\n",
      "tensor([[ 0.2487, -0.1586, -0.0324, -0.1829,  0.2976,  0.2827,  0.0244,  0.0458],\n",
      "        [-0.1828, -0.0399,  0.1755, -0.1914,  0.3137, -0.1701, -0.1310, -0.1482],\n",
      "        [-0.1335,  0.0220,  0.1088,  0.3528,  0.1897,  0.1915,  0.3194, -0.0247],\n",
      "        [ 0.1047,  0.0127,  0.3496,  0.3463,  0.3140, -0.2546,  0.1704,  0.1965],\n",
      "        [ 0.3424, -0.0777, -0.0440, -0.1257, -0.2182, -0.2159,  0.1930,  0.1487],\n",
      "        [ 0.0996,  0.1196, -0.1245, -0.0902,  0.1942, -0.1802, -0.2066,  0.2046],\n",
      "        [-0.0779,  0.1743,  0.0297, -0.1705, -0.0783, -0.0207,  0.0988, -0.3043],\n",
      "        [ 0.1671, -0.3463,  0.0973,  0.0537,  0.1802,  0.1356,  0.1058,  0.0933],\n",
      "        [ 0.2581,  0.3125,  0.3247, -0.1135, -0.3151, -0.2700, -0.3082,  0.2982],\n",
      "        [ 0.3083, -0.3372, -0.2963,  0.1225,  0.2294,  0.1120, -0.3493,  0.3369]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2376, -0.2277, -0.1175, -0.1717,  0.0766,  0.3398,  0.0819, -0.2916,\n",
      "        -0.2965,  0.0443], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.2072,  0.1883,  0.0132,  0.1531,  0.2527, -0.1449,  0.0414, -0.0605,\n",
      "          0.0848, -0.0933],\n",
      "        [-0.0079,  0.0459,  0.2113,  0.1116,  0.0390,  0.0613,  0.0428, -0.3044,\n",
      "          0.2998,  0.2027],\n",
      "        [-0.2049, -0.0228, -0.0571, -0.1981,  0.1895,  0.0271, -0.2713,  0.1179,\n",
      "         -0.3199, -0.1600],\n",
      "        [-0.2318, -0.2487,  0.1554, -0.1440,  0.1372, -0.0440, -0.2951,  0.2098,\n",
      "          0.1499, -0.1015],\n",
      "        [-0.2939, -0.1682, -0.0320, -0.0895,  0.1823, -0.1092,  0.0735, -0.0085,\n",
      "         -0.0638, -0.2945],\n",
      "        [ 0.2387, -0.0391,  0.1088,  0.2599,  0.2652,  0.1059,  0.1050,  0.3109,\n",
      "          0.0715,  0.0051],\n",
      "        [ 0.0762, -0.1613,  0.0342, -0.2316, -0.1122,  0.1362,  0.2193,  0.0400,\n",
      "         -0.3079, -0.2420],\n",
      "        [-0.1192,  0.2089, -0.1960, -0.2908, -0.2054, -0.0747,  0.0569,  0.0912,\n",
      "         -0.1567, -0.0954],\n",
      "        [ 0.2068,  0.1076, -0.2292,  0.3009,  0.1173, -0.3017, -0.1307, -0.0500,\n",
      "          0.0115,  0.2892],\n",
      "        [-0.0321,  0.2500,  0.1701, -0.3142,  0.1119, -0.0740,  0.2708,  0.2253,\n",
      "          0.2079, -0.2657]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.1639,  0.0764,  0.0961, -0.0039,  0.2518, -0.0064,  0.3155, -0.0528,\n",
      "        -0.1946, -0.0524], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.1174, -0.2764,  0.2248, -0.1552, -0.1439,  0.0999, -0.1719, -0.0167,\n",
      "          0.0130, -0.2149]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1083], requires_grad=True)]\n",
      "tensor([0.6308], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Despues\")\n",
    "print([i for i in red.parameters()])\n",
    "print(red(torch.Tensor([1,2,3,4,5,6,7,8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, h, w):\n",
    "        super(Net, self).__init__()\n",
    "        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.f2 = torch.nn.Linear(32 * h * w, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.f2(x)\n",
    "        return x\n",
    "\n",
    "class NARNN(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(NARNN, self).__init__()\n",
    "        #self.hidden_dim = hidden_dim\n",
    "        #self.num_layers = num_layers\n",
    "        #self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)#capa lstm\n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim)#capa lineal\n",
    "        self.fc1 = nn.Linear(input_dim,10)\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "        self.fc3 = nn.Linear(10,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()#Crea tensores con las dimensiones especificadas\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc1(out[:, -1, :]) \n",
    "        return out\"\"\"\n",
    "        tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))\n",
    "        x = tan_sigmoid(self.fc1(x))\n",
    "        #print(y)\n",
    "        #x = torch.sigmoid(self.fc1(x))\n",
    "        #print(x)\n",
    "        x = F.logsigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def fun(a, b, c, d):\n",
    "    p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]\n",
    "    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)\n",
    "    y = torch.randint(0, 5, [8])\n",
    "    x = F.conv2d(x, p[0], p[1], 1, 1)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.linear(x, p[2], p[3])\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    return loss\n",
    "\n",
    "red = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "input = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "\n",
    "print(input)\n",
    "entrada = input #la entrada se da como un parametro global\n",
    "salida_esperada = torch.tensor([-0.0834]) #lo mismo para la salida esperada\n",
    "λ = 0.1\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     #red.fc1.weight.data = \n",
    "#     params = []\n",
    "#     i=0\n",
    "#     for param in parametros:\n",
    "#         params.append(param)\n",
    "#         #print(param)\n",
    "#     for r_param in red.parameters():\n",
    "#         params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "#         r_param = params[i]\n",
    "#         i = i+1\n",
    "#     #print([b for b in red.parameters()])\n",
    "#     salida = red(entrada)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     return criterion(salida,salida_esperada)\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     #red.fc1.weight.data = \n",
    "#     params = []\n",
    "#     i=0\n",
    "#     #parametros = list(parametros)\n",
    "#     for param in parametros:\n",
    "#         params.append(param)\n",
    "#         print(param)\n",
    "#     for r_param in red.parameters():\n",
    "#         params[i] = params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "#         # r_param = params[i]\n",
    "#         i = i+1\n",
    "#     #print([b for b in red.parameters()]\n",
    "#     l1 = F.linear(entrada,params[0],params[1])\n",
    "#     #print(params[0])\n",
    "#     #print(params[1])\n",
    "#     #print(\"Entradal1: \" + str(l1))\n",
    "#     l2 = F.linear(l1,params[2],params[3])\n",
    "#     #print(\"Entradal2: \" + str(l2))\n",
    "#     salida = F.linear(l2,params[4],params[5])\n",
    "#     #print(\"Salida: \" + str(salida))\n",
    "\n",
    "#     #salida = (entrada)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     return criterion(salida,salida_esperada)\n",
    "\n",
    "def calcula_perdida(*parametros):\n",
    "    #red.fc1.weight.data = \n",
    "    params = []\n",
    "    n_params = []\n",
    "    i=0\n",
    "    n_dim = 0\n",
    "    #parametros = list(parametros)\n",
    "    for param in parametros:\n",
    "        params.append(param)#recibiria un solo tensor con todos los pesos\n",
    "    for r_param in red.parameters():\n",
    "        p_partida = n_dim\n",
    "        print(\"shape: \" + str(r_param.shape))\n",
    "        n_dim = r_param.size(0)*(r_param.size(1) if r_param.dim() == 2 else 1) + p_partida#se obtiene la dimension del primer conjunto de pparametros de la red\n",
    "        print(\"n_dim: \" + str(n_dim) + \" p_partida: \" + str(p_partida))\n",
    "        #print(\"primer vector: \" + str(params[0][p_partida:n_dim]))\n",
    "        n_params.append(params[0][p_partida:n_dim])\n",
    "        print(\"primer vector: \" + str(n_params[i]))\n",
    "        n_params[i] = n_params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "        print(\"segundo vector: \" + str(n_params[i]))\n",
    "        i = i+1\n",
    "    #print([b for b in red.parameters()]\n",
    "    l1 = F.linear(entrada,n_params[0],n_params[1])\n",
    "    #print(params[0])\n",
    "    #print(params[1])\n",
    "    #print(\"Entradal1: \" + str(l1))\n",
    "    l2 = F.linear(l1,n_params[2],n_params[3])\n",
    "    #print(\"Entradal2: \" + str(l2))\n",
    "    salida = F.linear(l2,n_params[4],n_params[5])\n",
    "    #print(\"Salida: \" + str(salida))\n",
    "\n",
    "    #salida = (entrada)\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(salida,salida_esperada)\n",
    "\n",
    "\n",
    "#print([b for b in red.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.rand(2,2)\n",
    "a= torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "print(a.dim())\n",
    "print(torch.rand(2,5))\n",
    "print(a.view(2,5))\n",
    "def pow_reducer(x):\n",
    "    return x.pow(3).sum()\n",
    "inputs = torch.rand(10)\n",
    "print(inputs.dim())\n",
    "print(torch.autograd.functional.hessian(pow_reducer, inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = input\n",
    "#print(red.fc1.weight.data)\n",
    "#red.fc1.weight.data = torch.zeros((10,8))\n",
    "#print(red.fc1.weight.data)\n",
    "#print(torch.tensor([[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8]]))\n",
    "#print(red.fc2.weight.data)\n",
    "#print(red.fc3.weight.data)\n",
    "v = [_.view(-1) for _ in red.parameters()]\n",
    "#for i in v:\n",
    " #   print(i)\n",
    "v = torch.cat([_.view(-1) for _ in red.parameters()], dim = 0)#concatena los paremetros de la red en un solo vector unidimensional\n",
    "#print(v)\n",
    "p = calcula_perdida(v)\n",
    "#print(len(v))\n",
    "#p = calcula_perdida(v[0],v[1],v[2],v[3],v[4],v[5])\n",
    "#print(red(entrada))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.dim())\n",
    "# class Net(Module):\n",
    "#     def __init__(self, h, w):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)\n",
    "#         self.f2 = torch.nn.Linear(32 * h * w, 5)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.c1(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.f2(x)\n",
    "#         return x\n",
    "\n",
    "# def haha(a, b, c, d):\n",
    "#     p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]\n",
    "#     x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)\n",
    "#     y = torch.randint(0, 5, [8])\n",
    "#     x = F.conv2d(x, p[0], p[1], 1, 1)\n",
    "#     x = x.view(x.size(0), -1)\n",
    "#     x = F.linear(x, p[2], p[3])\n",
    "#     loss = F.cross_entropy(x, y)\n",
    "#     return loss\n",
    "h = torch.autograd.functional.hessian(calcula_perdida, v)\n",
    "#print(len(list(a)))\n",
    "#print(len(list(a)[0]))\n",
    "print(h.dim())\n",
    "print(h.shape)\n",
    "\n",
    "#print(h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = torch.autograd.grad(calcula_perdida(v), v)[0]\n",
    "print(grad_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.1\n",
    "r = torch.matmul((-torch.inverse(h+λ*torch.eye(211))),torch.transpose(torch.unsqueeze(grad_f, 0),0, 1))\n",
    "print(r)\n",
    "print(torch.transpose(r,0,1))\n",
    "#-torch.inverse(h+λ*torch.eye(211))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)\n",
    "#print(torch.transpose(torch.unsqueeze(grad_f, 0),0, 1).shape)\n",
    "\n",
    "#print(-(0.1*torch.eye(211)+torch.eye(211)))\n",
    "#print([_.view(-1) for _ in red.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(1,80)\n",
    "print(x)\n",
    "y = []\n",
    "for i in red.parameters():\n",
    "    y.append(i.shape)\n",
    "#x = x.view(y[0])\n",
    "print(x.view(y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes dos tensores con diferentes formas\n",
    "tensor_existente = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "otro_tensor = torch.tensor([[7, 8], [9, 10],[11,12]])\n",
    "\n",
    "# Obtén la forma del otro tensor\n",
    "forma_deseada = otro_tensor.shape\n",
    "\n",
    "# Ajusta la forma del tensor existente para que coincida con la forma deseada\n",
    "tensor_ajustado = tensor_existente.view(forma_deseada)\n",
    "\n",
    "# Alternativamente, puedes usar reshape\n",
    "# tensor_ajustado = tensor_existente.reshape(forma_deseada)\n",
    "\n",
    "print(\"Tensor existente:\")\n",
    "print(tensor_existente)\n",
    "\n",
    "print(\"Tensor ajustado con la misma forma que el otro tensor:\")\n",
    "print(tensor_ajustado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_reducer(x):\n",
    "    x=x.view(-1)\n",
    "    return x.pow(3).sum()\n",
    "inputs = torch.Tensor([[2,4],[1,3]])\n",
    "print(inputs)\n",
    "torch.autograd.functional.hessian(pow_reducer, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.Tensor([[2,4],[1,3]]))\n",
    "print(torch.Tensor([[2,4],[1,3]]).view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_r(x):\n",
    "    return x.pow(3).sum()\n",
    "\n",
    "print(pow_r(torch.Tensor([2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(12, 12)\n",
    "\n",
    "print([i.view(-1) for i in net.parameters()][0].size())\n",
    "print([i for i in net.parameters()][0].size())\n",
    "\n",
    "t = tuple([_.view(-1) for _ in net.parameters()])\n",
    "\n",
    "print(torch.randn(size=[8, 1, 12, 12], dtype=torch.float32))\n",
    "#print(net)\n",
    "#print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()]))\n",
    "tuple([_.view(-1) for _ in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# v = torch.cat([_.view(-1) for _ in red.parameters()], dim = 0)#concatena los paremetros de la red en un solo vector unidimensional\n",
    "# h = torch.autograd.functional.hessian(lm.calcula_perdida, v) #calculamos la matriz hessiana\n",
    "# grad_f = torch.autograd.grad(lm.calcula_perdida(v), v)[0] #calculamos el gradiente de la funcion\n",
    "# r = -torch.inverse(h+λ*torch.eye(211))*torch.transpose(grad_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
