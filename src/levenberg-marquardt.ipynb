{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4884], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/miguel/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Module\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from NARNN import NARNN\n",
    "\n",
    "from levenberg_marquardt import LM\n",
    "\n",
    "red = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "#input = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "\n",
    "# print(input)\n",
    "entrada = input #la entrada se da como un parametro global\n",
    "salida_esperada = torch.tensor([-0.0834]) #lo mismo para la salida esperada\n",
    "# λ = 0.1\n",
    "\n",
    "\n",
    "\n",
    "print(red(torch.Tensor([1,2,3,4,5,6,7,8])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes\n",
      "[Parameter containing:\n",
      "tensor([[-0.0797,  0.0073, -0.0598,  0.1786, -0.2524,  0.0537,  0.2277, -0.1192],\n",
      "        [-0.1985,  0.2473,  0.0691, -0.0271,  0.1058,  0.0982, -0.1964, -0.1613],\n",
      "        [ 0.0789,  0.1483, -0.1321, -0.0894,  0.2940, -0.2954,  0.1811,  0.2621],\n",
      "        [-0.2028,  0.2626, -0.1991, -0.2144,  0.2224,  0.2139,  0.2694,  0.1281],\n",
      "        [ 0.0284, -0.0204,  0.1192,  0.2104, -0.0191, -0.0847, -0.3487, -0.2058],\n",
      "        [ 0.0412,  0.2255,  0.2777, -0.3318, -0.2635, -0.2094, -0.3100, -0.2829],\n",
      "        [-0.3337, -0.2234,  0.1577,  0.3484, -0.1335, -0.0568, -0.1739,  0.2567],\n",
      "        [ 0.1404, -0.0089, -0.1057, -0.1257, -0.2342, -0.0408,  0.3042, -0.0711],\n",
      "        [-0.1237, -0.0345, -0.2599,  0.1833,  0.1387, -0.2969, -0.0318, -0.2513],\n",
      "        [-0.1788,  0.1941,  0.3322,  0.3085, -0.2480, -0.0149,  0.1150, -0.0085]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2034,  0.2809, -0.0021, -0.1663, -0.3272,  0.1422,  0.2390, -0.1130,\n",
      "        -0.1114, -0.1568], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0716,  0.0703, -0.2236, -0.1895,  0.0386, -0.0282,  0.1429,  0.0309,\n",
      "         -0.2954,  0.3079],\n",
      "        [ 0.2493,  0.1758,  0.0965,  0.1749,  0.1460, -0.1735,  0.2040,  0.0921,\n",
      "         -0.2512, -0.0713],\n",
      "        [-0.0357,  0.2083,  0.1396,  0.1112, -0.0850, -0.2878, -0.2793, -0.0076,\n",
      "          0.1927,  0.2354],\n",
      "        [-0.2358, -0.0821,  0.1432, -0.2937,  0.0863,  0.0754,  0.2660, -0.1437,\n",
      "         -0.1203,  0.0661],\n",
      "        [ 0.1313,  0.1036,  0.1514,  0.1806, -0.2668, -0.0304,  0.2890,  0.0875,\n",
      "         -0.0655, -0.1172],\n",
      "        [ 0.0705, -0.1363, -0.1588,  0.1765,  0.1665, -0.2729,  0.2792, -0.2846,\n",
      "          0.0646, -0.2204],\n",
      "        [-0.1273,  0.0962,  0.2349,  0.1345,  0.2388, -0.2643,  0.1018, -0.0218,\n",
      "          0.1851,  0.2848],\n",
      "        [ 0.1369, -0.2609,  0.0820,  0.1083, -0.0295,  0.1991, -0.1900, -0.0313,\n",
      "         -0.2735, -0.2113],\n",
      "        [ 0.2790,  0.2301, -0.1287, -0.0031,  0.1239, -0.1574,  0.3025, -0.0104,\n",
      "         -0.1040, -0.1227],\n",
      "        [-0.2860,  0.1324, -0.2814, -0.2821,  0.0304, -0.2050, -0.0514,  0.2339,\n",
      "          0.2710, -0.2950]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2268, -0.2472,  0.0008, -0.3115, -0.1629,  0.0887,  0.1102,  0.1602,\n",
      "        -0.0810, -0.2564], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2365, -0.0298,  0.1491, -0.0752, -0.2793,  0.2249, -0.1013, -0.2184,\n",
      "         -0.1650, -0.1256]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2164], requires_grad=True)]\n",
      "tensor([0.4884], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Antes\")\n",
    "print([i for i in red.parameters()])\n",
    "print(red(torch.Tensor([1,2,3,4,5,6,7,8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aux_convierte_parametros\n",
      "asigna parametros\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 8x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11981/1136741244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#print(red(torch.Tensor([1,2,3,4,5,6,7,8])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0574\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0526\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0532\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0526\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0574\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0446\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0259\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0728\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0675\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Despues\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(red(torch.Tensor([1,2,3,4,5,6,7,8])))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Stock_Exchange_NN_PP/src/levenberg_marquardt.py\u001b[0m in \u001b[0;36mexec\u001b[0;34m(self, epocas)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepocas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mf_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcula_perdida\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_convierte_parametros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepocas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#print(\"epoca: \" + str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Stock_Exchange_NN_PP/src/levenberg_marquardt.py\u001b[0m in \u001b[0;36mcalcula_perdida\u001b[0;34m(self, *parametros)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#print(\"parametrosi: \" + str(entrada))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrada\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0msalida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 8x10)"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(red(torch.Tensor([1,2,3,4,5,6,7,8])))\n",
    "lm = LM(red, torch.Tensor([0.0574, 0.0526, 0.0532, 0.0526, 0.0574, 0.0446, 0.0259, 0.0728]),torch.tensor([0.0675]))\n",
    "lm.exec()\n",
    "print(\"Despues\")\n",
    "#print(red(torch.Tensor([1,2,3,4,5,6,7,8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despues\n",
      "[Parameter containing:\n",
      "tensor([[-0.0797,  0.0073, -0.0598,  0.1786, -0.2524,  0.0537,  0.2276, -0.1193],\n",
      "        [-0.1984,  0.2474,  0.0693, -0.0269,  0.1061,  0.0985, -0.1960, -0.1609],\n",
      "        [ 0.0788,  0.1481, -0.1325, -0.0899,  0.2934, -0.2962,  0.1803,  0.2611],\n",
      "        [-0.2030,  0.2622, -0.1998, -0.2153,  0.2213,  0.2126,  0.2678,  0.1263],\n",
      "        [ 0.0286, -0.0200,  0.1199,  0.2112, -0.0180, -0.0834, -0.3471, -0.2041],\n",
      "        [ 0.0417,  0.2263,  0.2790, -0.3301, -0.2614, -0.2069, -0.3070, -0.2795],\n",
      "        [-0.3338, -0.2235,  0.1576,  0.3482, -0.1338, -0.0571, -0.1743,  0.2563],\n",
      "        [ 0.1405, -0.0088, -0.1055, -0.1255, -0.2339, -0.0405,  0.3046, -0.0706],\n",
      "        [-0.1235, -0.0341, -0.2593,  0.1842,  0.1397, -0.2956, -0.0304, -0.2496],\n",
      "        [-0.1789,  0.1938,  0.3318,  0.3079, -0.2487, -0.0158,  0.1140, -0.0096]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.2034,  0.2810, -0.0023, -0.1665, -0.3270,  0.1427,  0.2389, -0.1129,\n",
      "        -0.1112, -0.1569], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0720,  0.0706, -0.2228, -0.1894,  0.0386, -0.0269,  0.1435,  0.0318,\n",
      "         -0.2953,  0.3062],\n",
      "        [ 0.2486,  0.1772,  0.0916,  0.1678,  0.1528, -0.1614,  0.2019,  0.0932,\n",
      "         -0.2447, -0.0741],\n",
      "        [-0.0360,  0.2100,  0.1355,  0.1044, -0.0785, -0.2752, -0.2809, -0.0059,\n",
      "          0.1991,  0.2315],\n",
      "        [-0.2357, -0.0829,  0.1451, -0.2904,  0.0832,  0.0694,  0.2667, -0.1445,\n",
      "         -0.1233,  0.0680],\n",
      "        [ 0.1299,  0.1050,  0.1449,  0.1723, -0.2590, -0.0179,  0.2860,  0.0879,\n",
      "         -0.0581, -0.1188],\n",
      "        [ 0.0706, -0.1349, -0.1612,  0.1717,  0.1712, -0.2633,  0.2784, -0.2829,\n",
      "          0.0692, -0.2241],\n",
      "        [-0.1281,  0.0974,  0.2303,  0.1282,  0.2448, -0.2541,  0.0997, -0.0212,\n",
      "          0.1908,  0.2829],\n",
      "        [ 0.1365, -0.2609,  0.0802,  0.1069, -0.0281,  0.2002, -0.1909, -0.0318,\n",
      "         -0.2723, -0.2104],\n",
      "        [ 0.2784,  0.2304, -0.1307, -0.0053,  0.1259, -0.1547,  0.3015, -0.0106,\n",
      "         -0.1021, -0.1224],\n",
      "        [-0.2854,  0.1304, -0.2761, -0.2734,  0.0221, -0.2206, -0.0493,  0.2320,\n",
      "          0.2630, -0.2905]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2287, -0.2487, -0.0021, -0.3101, -0.1627,  0.0857,  0.1094,  0.1616,\n",
      "        -0.0802, -0.2532], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2171, -0.0276,  0.1366, -0.0688, -0.2567,  0.2061, -0.0932, -0.2005,\n",
      "         -0.1515, -0.1148]], requires_grad=True), Parameter containing:\n",
      "tensor([8.9578], requires_grad=True)]\n",
      "tensor([9.2060], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Despues\")\n",
    "print([i for i in red.parameters()])\n",
    "print(red(torch.Tensor([1,2,3,4,5,6,7,8])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, h, w):\n",
    "        super(Net, self).__init__()\n",
    "        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)\n",
    "        self.f2 = torch.nn.Linear(32 * h * w, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.f2(x)\n",
    "        return x\n",
    "\n",
    "class NARNN(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(NARNN, self).__init__()\n",
    "        #self.hidden_dim = hidden_dim\n",
    "        #self.num_layers = num_layers\n",
    "        #self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)#capa lstm\n",
    "        #self.fc1 = nn.Linear(hidden_dim, output_dim)#capa lineal\n",
    "        self.fc1 = nn.Linear(input_dim,10)\n",
    "        self.fc2 = nn.Linear(10,10)\n",
    "        self.fc3 = nn.Linear(10,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()#Crea tensores con las dimensiones especificadas\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.fc1(out[:, -1, :]) \n",
    "        return out\"\"\"\n",
    "        tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))\n",
    "        x = tan_sigmoid(self.fc1(x))\n",
    "        #print(y)\n",
    "        #x = torch.sigmoid(self.fc1(x))\n",
    "        #print(x)\n",
    "        x = F.logsigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def fun(a, b, c, d):\n",
    "    p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]\n",
    "    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)\n",
    "    y = torch.randint(0, 5, [8])\n",
    "    x = F.conv2d(x, p[0], p[1], 1, 1)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.linear(x, p[2], p[3])\n",
    "    loss = F.cross_entropy(x, y)\n",
    "    return loss\n",
    "\n",
    "red = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)\n",
    "input = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "\n",
    "print(input)\n",
    "entrada = input #la entrada se da como un parametro global\n",
    "salida_esperada = torch.tensor([-0.0834]) #lo mismo para la salida esperada\n",
    "λ = 0.1\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     #red.fc1.weight.data = \n",
    "#     params = []\n",
    "#     i=0\n",
    "#     for param in parametros:\n",
    "#         params.append(param)\n",
    "#         #print(param)\n",
    "#     for r_param in red.parameters():\n",
    "#         params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "#         r_param = params[i]\n",
    "#         i = i+1\n",
    "#     #print([b for b in red.parameters()])\n",
    "#     salida = red(entrada)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     return criterion(salida,salida_esperada)\n",
    "\n",
    "# def calcula_perdida(*parametros):\n",
    "#     #red.fc1.weight.data = \n",
    "#     params = []\n",
    "#     i=0\n",
    "#     #parametros = list(parametros)\n",
    "#     for param in parametros:\n",
    "#         params.append(param)\n",
    "#         print(param)\n",
    "#     for r_param in red.parameters():\n",
    "#         params[i] = params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "#         # r_param = params[i]\n",
    "#         i = i+1\n",
    "#     #print([b for b in red.parameters()]\n",
    "#     l1 = F.linear(entrada,params[0],params[1])\n",
    "#     #print(params[0])\n",
    "#     #print(params[1])\n",
    "#     #print(\"Entradal1: \" + str(l1))\n",
    "#     l2 = F.linear(l1,params[2],params[3])\n",
    "#     #print(\"Entradal2: \" + str(l2))\n",
    "#     salida = F.linear(l2,params[4],params[5])\n",
    "#     #print(\"Salida: \" + str(salida))\n",
    "\n",
    "#     #salida = (entrada)\n",
    "#     criterion = nn.MSELoss()\n",
    "#     return criterion(salida,salida_esperada)\n",
    "\n",
    "def calcula_perdida(*parametros):\n",
    "    #red.fc1.weight.data = \n",
    "    params = []\n",
    "    n_params = []\n",
    "    i=0\n",
    "    n_dim = 0\n",
    "    #parametros = list(parametros)\n",
    "    for param in parametros:\n",
    "        params.append(param)#recibiria un solo tensor con todos los pesos\n",
    "    for r_param in red.parameters():\n",
    "        p_partida = n_dim\n",
    "        print(\"shape: \" + str(r_param.shape))\n",
    "        n_dim = r_param.size(0)*(r_param.size(1) if r_param.dim() == 2 else 1) + p_partida#se obtiene la dimension del primer conjunto de pparametros de la red\n",
    "        print(\"n_dim: \" + str(n_dim) + \" p_partida: \" + str(p_partida))\n",
    "        #print(\"primer vector: \" + str(params[0][p_partida:n_dim]))\n",
    "        n_params.append(params[0][p_partida:n_dim])\n",
    "        print(\"primer vector: \" + str(n_params[i]))\n",
    "        n_params[i] = n_params[i].view(r_param.shape)#se le da la forma del parametro correspondiente a los parametros que llegan como entrada\n",
    "        print(\"segundo vector: \" + str(n_params[i]))\n",
    "        i = i+1\n",
    "    #print([b for b in red.parameters()]\n",
    "    l1 = F.linear(entrada,n_params[0],n_params[1])\n",
    "    #print(params[0])\n",
    "    #print(params[1])\n",
    "    #print(\"Entradal1: \" + str(l1))\n",
    "    l2 = F.linear(l1,n_params[2],n_params[3])\n",
    "    #print(\"Entradal2: \" + str(l2))\n",
    "    salida = F.linear(l2,n_params[4],n_params[5])\n",
    "    #print(\"Salida: \" + str(salida))\n",
    "\n",
    "    #salida = (entrada)\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(salida,salida_esperada)\n",
    "\n",
    "\n",
    "#print([b for b in red.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.rand(2,2)\n",
    "a= torch.tensor([1,2,3,4,5,6,7,8,9,10])\n",
    "print(a.dim())\n",
    "print(torch.rand(2,5))\n",
    "print(a.view(2,5))\n",
    "def pow_reducer(x):\n",
    "    return x.pow(3).sum()\n",
    "inputs = torch.rand(10)\n",
    "print(inputs.dim())\n",
    "print(torch.autograd.functional.hessian(pow_reducer, inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada = input\n",
    "#print(red.fc1.weight.data)\n",
    "#red.fc1.weight.data = torch.zeros((10,8))\n",
    "#print(red.fc1.weight.data)\n",
    "#print(torch.tensor([[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8]]))\n",
    "#print(red.fc2.weight.data)\n",
    "#print(red.fc3.weight.data)\n",
    "v = [_.view(-1) for _ in red.parameters()]\n",
    "#for i in v:\n",
    " #   print(i)\n",
    "v = torch.cat([_.view(-1) for _ in red.parameters()], dim = 0)#concatena los paremetros de la red en un solo vector unidimensional\n",
    "#print(v)\n",
    "p = calcula_perdida(v)\n",
    "#print(len(v))\n",
    "#p = calcula_perdida(v[0],v[1],v[2],v[3],v[4],v[5])\n",
    "#print(red(entrada))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.dim())\n",
    "# class Net(Module):\n",
    "#     def __init__(self, h, w):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)\n",
    "#         self.f2 = torch.nn.Linear(32 * h * w, 5)\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.c1(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.f2(x)\n",
    "#         return x\n",
    "\n",
    "# def haha(a, b, c, d):\n",
    "#     p = [a.view(32, 1, 3, 3), b, c.view(5, 32 * 12 * 12), d]\n",
    "#     x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)\n",
    "#     y = torch.randint(0, 5, [8])\n",
    "#     x = F.conv2d(x, p[0], p[1], 1, 1)\n",
    "#     x = x.view(x.size(0), -1)\n",
    "#     x = F.linear(x, p[2], p[3])\n",
    "#     loss = F.cross_entropy(x, y)\n",
    "#     return loss\n",
    "h = torch.autograd.functional.hessian(calcula_perdida, v)\n",
    "#print(len(list(a)))\n",
    "#print(len(list(a)[0]))\n",
    "print(h.dim())\n",
    "print(h.shape)\n",
    "\n",
    "#print(h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = torch.autograd.grad(calcula_perdida(v), v)[0]\n",
    "print(grad_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 0.1\n",
    "r = torch.matmul((-torch.inverse(h+λ*torch.eye(211))),torch.transpose(torch.unsqueeze(grad_f, 0),0, 1))\n",
    "print(r)\n",
    "print(torch.transpose(r,0,1))\n",
    "#-torch.inverse(h+λ*torch.eye(211))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)\n",
    "#print(torch.transpose(torch.unsqueeze(grad_f, 0),0, 1).shape)\n",
    "\n",
    "#print(-(0.1*torch.eye(211)+torch.eye(211)))\n",
    "#print([_.view(-1) for _ in red.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.randn(1,80)\n",
    "print(x)\n",
    "y = []\n",
    "for i in red.parameters():\n",
    "    y.append(i.shape)\n",
    "#x = x.view(y[0])\n",
    "print(x.view(y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos que tienes dos tensores con diferentes formas\n",
    "tensor_existente = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "otro_tensor = torch.tensor([[7, 8], [9, 10],[11,12]])\n",
    "\n",
    "# Obtén la forma del otro tensor\n",
    "forma_deseada = otro_tensor.shape\n",
    "\n",
    "# Ajusta la forma del tensor existente para que coincida con la forma deseada\n",
    "tensor_ajustado = tensor_existente.view(forma_deseada)\n",
    "\n",
    "# Alternativamente, puedes usar reshape\n",
    "# tensor_ajustado = tensor_existente.reshape(forma_deseada)\n",
    "\n",
    "print(\"Tensor existente:\")\n",
    "print(tensor_existente)\n",
    "\n",
    "print(\"Tensor ajustado con la misma forma que el otro tensor:\")\n",
    "print(tensor_ajustado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_reducer(x):\n",
    "    x=x.view(-1)\n",
    "    return x.pow(3).sum()\n",
    "inputs = torch.Tensor([[2,4],[1,3]])\n",
    "print(inputs)\n",
    "torch.autograd.functional.hessian(pow_reducer, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.Tensor([[2,4],[1,3]]))\n",
    "print(torch.Tensor([[2,4],[1,3]]).view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_r(x):\n",
    "    return x.pow(3).sum()\n",
    "\n",
    "print(pow_r(torch.Tensor([2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(12, 12)\n",
    "\n",
    "print([i.view(-1) for i in net.parameters()][0].size())\n",
    "print([i for i in net.parameters()][0].size())\n",
    "\n",
    "t = tuple([_.view(-1) for _ in net.parameters()])\n",
    "\n",
    "print(torch.randn(size=[8, 1, 12, 12], dtype=torch.float32))\n",
    "#print(net)\n",
    "#print(len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#h = torch.autograd.functional.hessian(haha, tuple([_.view(-1) for _ in net.parameters()]))\n",
    "tuple([_.view(-1) for _ in net.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# v = torch.cat([_.view(-1) for _ in red.parameters()], dim = 0)#concatena los paremetros de la red en un solo vector unidimensional\n",
    "# h = torch.autograd.functional.hessian(lm.calcula_perdida, v) #calculamos la matriz hessiana\n",
    "# grad_f = torch.autograd.grad(lm.calcula_perdida(v), v)[0] #calculamos el gradiente de la funcion\n",
    "# r = -torch.inverse(h+λ*torch.eye(211))*torch.transpose(grad_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
