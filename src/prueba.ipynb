{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.loss.MSELoss'>\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from NARNN import NARNN\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(type(criterion))\n",
    "red_A1 = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: tensor([[ 0.1365,  0.1475,  0.0727, -0.2675, -0.3428, -0.2591, -0.3029, -0.1742],\n",
      "        [-0.1430, -0.0163, -0.3324, -0.3178,  0.1835, -0.2752, -0.1677, -0.3370],\n",
      "        [ 0.2560,  0.0464, -0.0847,  0.1569,  0.1387, -0.0720, -0.2338, -0.0957],\n",
      "        [ 0.3159,  0.2725,  0.3294, -0.1795,  0.1986, -0.0058, -0.0068,  0.1954],\n",
      "        [ 0.1210,  0.1950,  0.0539,  0.1472, -0.2765, -0.1109, -0.0905,  0.0989],\n",
      "        [ 0.3171, -0.1050,  0.1900, -0.1380, -0.0681, -0.1221,  0.0766,  0.1762],\n",
      "        [-0.2288,  0.2587, -0.1251,  0.2546, -0.1502, -0.1607, -0.1101,  0.0111],\n",
      "        [-0.1900, -0.3452, -0.1824, -0.2005,  0.1285, -0.2236,  0.0646, -0.0204],\n",
      "        [-0.1820, -0.3059, -0.2245, -0.2715,  0.0053, -0.0027, -0.2166, -0.0728],\n",
      "        [ 0.3393,  0.3314,  0.0455,  0.2558,  0.0813,  0.2907,  0.1761, -0.2713]])\n",
      "param: tensor([ 0.2669, -0.1122,  0.0157,  0.2356, -0.2959,  0.2585,  0.1946,  0.0431,\n",
      "         0.1356,  0.3215])\n",
      "param: tensor([[ 0.0075, -0.0612, -0.2975, -0.1153, -0.2520, -0.1653, -0.0044,  0.2366,\n",
      "          0.2381, -0.1082],\n",
      "        [ 0.0499, -0.1392, -0.0748, -0.2410,  0.1902,  0.0036,  0.2252,  0.0812,\n",
      "          0.2076, -0.3067],\n",
      "        [ 0.1363, -0.3042, -0.3012,  0.1580,  0.1708, -0.0279,  0.1339, -0.0568,\n",
      "          0.2788,  0.1209],\n",
      "        [-0.2381,  0.1499, -0.2567,  0.1700, -0.1803,  0.2126,  0.0609, -0.2318,\n",
      "          0.3097, -0.2552],\n",
      "        [-0.1005,  0.2277, -0.2723, -0.3009, -0.2213,  0.1137, -0.1935,  0.1555,\n",
      "         -0.2520,  0.2279],\n",
      "        [ 0.3111, -0.2202,  0.1740,  0.0640, -0.0214, -0.0351,  0.2956,  0.3090,\n",
      "         -0.0307, -0.2699],\n",
      "        [-0.0625,  0.2827, -0.1311, -0.1716, -0.1687, -0.1767, -0.1975,  0.0190,\n",
      "         -0.2109,  0.1619],\n",
      "        [ 0.0598,  0.0722,  0.1043, -0.3126, -0.1883, -0.1465,  0.2124, -0.2978,\n",
      "          0.2736,  0.0792],\n",
      "        [ 0.0223, -0.2810, -0.0811,  0.2002, -0.0547, -0.2817, -0.2942, -0.1238,\n",
      "          0.0425,  0.0837],\n",
      "        [-0.1389,  0.2331, -0.0849,  0.1311,  0.2380, -0.0971, -0.1207, -0.0549,\n",
      "          0.3138,  0.2967]])\n",
      "param: tensor([ 0.2312,  0.0299,  0.2660,  0.0859, -0.0250,  0.2328, -0.1451,  0.1421,\n",
      "        -0.0910,  0.2670])\n",
      "param: tensor([[-0.3001,  0.0338,  0.3021,  0.2334, -0.0093, -0.1348,  0.1112,  0.2683,\n",
      "          0.2937,  0.1320]])\n",
      "param: tensor([0.1617])\n"
     ]
    }
   ],
   "source": [
    "for p in red_A1.parameters():\n",
    "    print(\"param: \" + str(p.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1365,  0.1475,  0.0727, -0.2675, -0.3428, -0.2591, -0.3029, -0.1742],\n",
      "        [-0.1430, -0.0163, -0.3324, -0.3178,  0.1835, -0.2752, -0.1677, -0.3370],\n",
      "        [ 0.2560,  0.0464, -0.0847,  0.1569,  0.1387, -0.0720, -0.2338, -0.0957],\n",
      "        [ 0.3159,  0.2725,  0.3294, -0.1795,  0.1986, -0.0058, -0.0068,  0.1954],\n",
      "        [ 0.1210,  0.1950,  0.0539,  0.1472, -0.2765, -0.1109, -0.0905,  0.0989],\n",
      "        [ 0.3171, -0.1050,  0.1900, -0.1380, -0.0681, -0.1221,  0.0766,  0.1762],\n",
      "        [-0.2288,  0.2587, -0.1251,  0.2546, -0.1502, -0.1607, -0.1101,  0.0111],\n",
      "        [-0.1900, -0.3452, -0.1824, -0.2005,  0.1285, -0.2236,  0.0646, -0.0204],\n",
      "        [-0.1820, -0.3059, -0.2245, -0.2715,  0.0053, -0.0027, -0.2166, -0.0728],\n",
      "        [ 0.3393,  0.3314,  0.0455,  0.2558,  0.0813,  0.2907,  0.1761, -0.2713]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2669, -0.1122,  0.0157,  0.2356, -0.2959,  0.2585,  0.1946,  0.0431,\n",
      "         0.1356,  0.3215], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0075, -0.0612, -0.2975, -0.1153, -0.2520, -0.1653, -0.0044,  0.2366,\n",
      "          0.2381, -0.1082],\n",
      "        [ 0.0499, -0.1392, -0.0748, -0.2410,  0.1902,  0.0036,  0.2252,  0.0812,\n",
      "          0.2076, -0.3067],\n",
      "        [ 0.1363, -0.3042, -0.3012,  0.1580,  0.1708, -0.0279,  0.1339, -0.0568,\n",
      "          0.2788,  0.1209],\n",
      "        [-0.2381,  0.1499, -0.2567,  0.1700, -0.1803,  0.2126,  0.0609, -0.2318,\n",
      "          0.3097, -0.2552],\n",
      "        [-0.1005,  0.2277, -0.2723, -0.3009, -0.2213,  0.1137, -0.1935,  0.1555,\n",
      "         -0.2520,  0.2279],\n",
      "        [ 0.3111, -0.2202,  0.1740,  0.0640, -0.0214, -0.0351,  0.2956,  0.3090,\n",
      "         -0.0307, -0.2699],\n",
      "        [-0.0625,  0.2827, -0.1311, -0.1716, -0.1687, -0.1767, -0.1975,  0.0190,\n",
      "         -0.2109,  0.1619],\n",
      "        [ 0.0598,  0.0722,  0.1043, -0.3126, -0.1883, -0.1465,  0.2124, -0.2978,\n",
      "          0.2736,  0.0792],\n",
      "        [ 0.0223, -0.2810, -0.0811,  0.2002, -0.0547, -0.2817, -0.2942, -0.1238,\n",
      "          0.0425,  0.0837],\n",
      "        [-0.1389,  0.2331, -0.0849,  0.1311,  0.2380, -0.0971, -0.1207, -0.0549,\n",
      "          0.3138,  0.2967]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2312,  0.0299,  0.2660,  0.0859, -0.0250,  0.2328, -0.1451,  0.1421,\n",
      "        -0.0910,  0.2670], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3001,  0.0338,  0.3021,  0.2334, -0.0093, -0.1348,  0.1112,  0.2683,\n",
      "          0.2937,  0.1320]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1617], requires_grad=True)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "def my_MSEloss(params):\n",
    "    c = 0\n",
    "    for param in red_A1.parameters():\n",
    "        param = params[c]\n",
    "        print(param)\n",
    "        c = c + 1\n",
    "\n",
    "for j in red_A1.parameters():\n",
    "    print(j)\n",
    "\n",
    "my_MSEloss([torch.zeros_like(tensor) for tensor in red_A1.parameters()])\n",
    "#for i in red_A1.parameters():\n",
    " #   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of NARNN(\n",
      "  (fc1): Linear(in_features=8, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(red_A1.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5007, 0.4139],\n",
      "        [0.9362, 0.1404]])\n",
      "tensor([0.3402, 0.1389])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.2854, 0.0563])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(2, 2))\n",
    "\n",
    "for i in torch.rand(2, 2):\n",
    "    print(i)\n",
    "    print(type(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dada = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "dz/dbda = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "dz/dadb = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "dz/dbdb = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor([1.0, -1.0, 2.0]))\n",
    "        self.b = torch.nn.Parameter(torch.tensor([2.0, -2.0, -1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = (x * self.a.unsqueeze(0) ** 3 + x * self.b.unsqueeze(0) ** 3).sum(dim=1)\n",
    "        return output\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = torch.tensor([1.0, 2.0])\n",
    "net = Net()\n",
    "\n",
    "def compute_z(*net_parameters):\n",
    "    # Is there a proper way to set the parameters that works with hessian?\n",
    "    for p_src, p_dst in zip(net_parameters, net.parameters()):\n",
    "        p_dst.data = p_src.data\n",
    "\n",
    "    output = net(x)\n",
    "    z = ((output - y)**2).mean()\n",
    "    return z\n",
    "\n",
    "# strict=True raises exception, allowing non-strict for demonstration\n",
    "hessians = torch.autograd.functional.hessian(compute_z, tuple(net.parameters()))\n",
    "\n",
    "param_names = [n for n, _ in net.named_parameters()]\n",
    "for d_name, d_hessians in zip(param_names, hessians):\n",
    "    for dd_name, dd_hessian in zip(param_names, d_hessians):\n",
    "        print(f'dz/d{dd_name}d{d_name} = \\n{dd_hessian}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.6509, grad_fn=<MseLossBackward0>)\n",
      "<class 'torch.nn.modules.loss.MSELoss'>\n",
      "<class 'MseLossBackward0'>\n",
      "tensor([-0.4397], grad_fn=<AddBackward0>)\n",
      "<class 'AddBackward0'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30660/2333174848.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# print(t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m     res = jacobian(jac_func, inputs, create_graph=create_graph, strict=strict, vectorize=vectorize,\n\u001b[0m\u001b[1;32m    808\u001b[0m                    strategy=outer_jacobian_strategy)\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tuple_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    576\u001b[0m                                               \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;31m# or else the input will be detached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jacobian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    576\u001b[0m                                               \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0mis_out_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hessian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#optimizer = optim.SGD(red.parameters(), lr=0.1, momentum=0.4)#,maximize=True)\n",
    "#optimizer.zero_grad()\n",
    "output = red_A1(torch.Tensor([1,2,3,4,5,6,7,8]))\n",
    "target = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "print(type(criterion))\n",
    "print(type(loss.grad_fn))\n",
    "\n",
    "print(output)\n",
    "print(type(output.grad_fn))\n",
    "\n",
    "t = []\n",
    "\n",
    "for c in red_A1.parameters():\n",
    "    print(type(c.data))\n",
    "    t.append(c.data)\n",
    "\n",
    "#torch.cat(,dim = 0)\n",
    "# print(t)\n",
    "\n",
    "h = torch.autograd.functional.hessian(loss.grad_fn, torch.rand(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3211808025.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_30660/3211808025.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    h+ lambda\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(loss)\n",
    "loss.backward()\n",
    "h = torch.autograd.functional.hessian(loss)#calcula el hessiana de la funcion de perdida\n",
    "h+ lambda \n",
    "print(loss.backward())\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
