{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.loss.MSELoss'>\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from NARNN import NARNN\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(type(criterion))\n",
    "red_A1 = NARNN(input_dim=8, hidden_dim=0, output_dim=1, num_layers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param: tensor([[ 0.2783,  0.2227,  0.2038,  0.1187, -0.0113,  0.1069,  0.2319,  0.0739],\n",
      "        [-0.2294,  0.1116,  0.3521, -0.2191,  0.0937,  0.1413,  0.0130, -0.2106],\n",
      "        [-0.1620,  0.1835,  0.1033, -0.2904,  0.0238,  0.0962,  0.2490,  0.0155],\n",
      "        [-0.2153,  0.2543,  0.0593,  0.1017,  0.2695,  0.3012,  0.1410,  0.0133],\n",
      "        [ 0.0976, -0.2911,  0.3376, -0.3454,  0.1768, -0.3010, -0.2370,  0.2148],\n",
      "        [-0.2219, -0.0692,  0.3108,  0.1200,  0.2536,  0.0757,  0.2455, -0.1522],\n",
      "        [-0.0592, -0.1702,  0.3058, -0.2832, -0.2144,  0.2021,  0.0814, -0.3482],\n",
      "        [-0.0352, -0.2680,  0.1825,  0.2927, -0.2879,  0.2107,  0.2917,  0.0661],\n",
      "        [ 0.1063,  0.0721,  0.0807,  0.0172,  0.1377,  0.2261,  0.3244, -0.3215],\n",
      "        [-0.0103,  0.1867, -0.1124,  0.3210, -0.2241, -0.0999, -0.2604, -0.3089]])\n",
      "param: tensor([ 0.1069, -0.0916,  0.3147, -0.1903, -0.1747, -0.3330,  0.1276, -0.0919,\n",
      "         0.0906, -0.1138])\n",
      "param: tensor([[-0.0738, -0.0498, -0.2000, -0.0822, -0.0509,  0.0750, -0.1564,  0.1849,\n",
      "          0.0938,  0.0171],\n",
      "        [ 0.3108,  0.1798, -0.2807, -0.2108,  0.0446, -0.1666,  0.1514,  0.0800,\n",
      "         -0.2671,  0.1661],\n",
      "        [ 0.2259,  0.1538, -0.2504, -0.2190, -0.1558,  0.2974,  0.2450, -0.1594,\n",
      "         -0.0592,  0.2981],\n",
      "        [-0.1319,  0.2405,  0.2799, -0.3148,  0.0828, -0.3069, -0.0860, -0.2891,\n",
      "         -0.1799, -0.2753],\n",
      "        [-0.0526, -0.0463, -0.1046, -0.0603,  0.1456,  0.2001,  0.0027, -0.1849,\n",
      "         -0.2720,  0.0304],\n",
      "        [ 0.2302, -0.0714,  0.0191,  0.2558, -0.1397,  0.1323, -0.2636, -0.0626,\n",
      "          0.1681,  0.0853],\n",
      "        [-0.0746, -0.2748, -0.1908,  0.2638,  0.2453,  0.3067,  0.1904, -0.2225,\n",
      "         -0.0102, -0.1304],\n",
      "        [ 0.1255, -0.0372, -0.2432, -0.1192,  0.0836,  0.1646,  0.0005, -0.0847,\n",
      "         -0.1555, -0.0610],\n",
      "        [-0.2845,  0.0419,  0.2160, -0.1218, -0.0557,  0.1255, -0.1571,  0.1873,\n",
      "         -0.3132,  0.0868],\n",
      "        [-0.0795,  0.1548, -0.0765,  0.0644,  0.1660,  0.1326, -0.1317, -0.2830,\n",
      "          0.2728,  0.2033]])\n",
      "param: tensor([-0.0515, -0.0857, -0.3006,  0.0869,  0.1518, -0.1423,  0.1204,  0.1829,\n",
      "        -0.2353, -0.0441])\n",
      "param: tensor([[-0.0184, -0.1532,  0.2582,  0.2652,  0.2718, -0.2101, -0.0507,  0.0206,\n",
      "          0.0072, -0.0206]])\n",
      "param: tensor([0.0321])\n"
     ]
    }
   ],
   "source": [
    "for p in red_A1.parameters():\n",
    "    print(\"param: \" + str(p.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2783,  0.2227,  0.2038,  0.1187, -0.0113,  0.1069,  0.2319,  0.0739],\n",
      "        [-0.2294,  0.1116,  0.3521, -0.2191,  0.0937,  0.1413,  0.0130, -0.2106],\n",
      "        [-0.1620,  0.1835,  0.1033, -0.2904,  0.0238,  0.0962,  0.2490,  0.0155],\n",
      "        [-0.2153,  0.2543,  0.0593,  0.1017,  0.2695,  0.3012,  0.1410,  0.0133],\n",
      "        [ 0.0976, -0.2911,  0.3376, -0.3454,  0.1768, -0.3010, -0.2370,  0.2148],\n",
      "        [-0.2219, -0.0692,  0.3108,  0.1200,  0.2536,  0.0757,  0.2455, -0.1522],\n",
      "        [-0.0592, -0.1702,  0.3058, -0.2832, -0.2144,  0.2021,  0.0814, -0.3482],\n",
      "        [-0.0352, -0.2680,  0.1825,  0.2927, -0.2879,  0.2107,  0.2917,  0.0661],\n",
      "        [ 0.1063,  0.0721,  0.0807,  0.0172,  0.1377,  0.2261,  0.3244, -0.3215],\n",
      "        [-0.0103,  0.1867, -0.1124,  0.3210, -0.2241, -0.0999, -0.2604, -0.3089]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1069, -0.0916,  0.3147, -0.1903, -0.1747, -0.3330,  0.1276, -0.0919,\n",
      "         0.0906, -0.1138], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0738, -0.0498, -0.2000, -0.0822, -0.0509,  0.0750, -0.1564,  0.1849,\n",
      "          0.0938,  0.0171],\n",
      "        [ 0.3108,  0.1798, -0.2807, -0.2108,  0.0446, -0.1666,  0.1514,  0.0800,\n",
      "         -0.2671,  0.1661],\n",
      "        [ 0.2259,  0.1538, -0.2504, -0.2190, -0.1558,  0.2974,  0.2450, -0.1594,\n",
      "         -0.0592,  0.2981],\n",
      "        [-0.1319,  0.2405,  0.2799, -0.3148,  0.0828, -0.3069, -0.0860, -0.2891,\n",
      "         -0.1799, -0.2753],\n",
      "        [-0.0526, -0.0463, -0.1046, -0.0603,  0.1456,  0.2001,  0.0027, -0.1849,\n",
      "         -0.2720,  0.0304],\n",
      "        [ 0.2302, -0.0714,  0.0191,  0.2558, -0.1397,  0.1323, -0.2636, -0.0626,\n",
      "          0.1681,  0.0853],\n",
      "        [-0.0746, -0.2748, -0.1908,  0.2638,  0.2453,  0.3067,  0.1904, -0.2225,\n",
      "         -0.0102, -0.1304],\n",
      "        [ 0.1255, -0.0372, -0.2432, -0.1192,  0.0836,  0.1646,  0.0005, -0.0847,\n",
      "         -0.1555, -0.0610],\n",
      "        [-0.2845,  0.0419,  0.2160, -0.1218, -0.0557,  0.1255, -0.1571,  0.1873,\n",
      "         -0.3132,  0.0868],\n",
      "        [-0.0795,  0.1548, -0.0765,  0.0644,  0.1660,  0.1326, -0.1317, -0.2830,\n",
      "          0.2728,  0.2033]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0515, -0.0857, -0.3006,  0.0869,  0.1518, -0.1423,  0.1204,  0.1829,\n",
      "        -0.2353, -0.0441], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0184, -0.1532,  0.2582,  0.2652,  0.2718, -0.2101, -0.0507,  0.0206,\n",
      "          0.0072, -0.0206]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0321], requires_grad=True)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "def my_MSEloss(params):\n",
    "    c = 0\n",
    "    for param in red_A1.parameters():\n",
    "        param = params[c]\n",
    "        print(param)\n",
    "        c = c + 1\n",
    "\n",
    "for j in red_A1.parameters():\n",
    "    print(j)\n",
    "\n",
    "my_MSEloss([torch.zeros_like(tensor) for tensor in red_A1.parameters()])\n",
    "#for i in red_A1.parameters():\n",
    " #   print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of NARNN(\n",
      "  (fc1): Linear(in_features=8, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "print(red_A1.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7509, 0.7377],\n",
      "        [0.2890, 0.5863]])\n",
      "tensor([0.4134, 0.6993])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([0.9921, 0.9088])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(2, 2))\n",
    "\n",
    "for i in torch.rand(2, 2):\n",
    "    print(i)\n",
    "    print(type(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dada = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "dz/dbda = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "dz/dadb = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "dz/dbdb = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.tensor([1.0, -1.0, 2.0]))\n",
    "        self.b = torch.nn.Parameter(torch.tensor([2.0, -2.0, -1.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = (x * self.a.unsqueeze(0) ** 3 + x * self.b.unsqueeze(0) ** 3).sum(dim=1)\n",
    "        return output\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = torch.tensor([1.0, 2.0])\n",
    "net = Net()\n",
    "\n",
    "def compute_z(*net_parameters):\n",
    "    # Is there a proper way to set the parameters that works with hessian?\n",
    "    for p_src, p_dst in zip(net_parameters, net.parameters()):\n",
    "        p_dst.data = p_src.data\n",
    "\n",
    "    output = net(x)\n",
    "    z = ((output - y)**2).mean()\n",
    "    return z\n",
    "\n",
    "# strict=True raises exception, allowing non-strict for demonstration\n",
    "hessians = torch.autograd.functional.hessian(compute_z, tuple(net.parameters()))\n",
    "\n",
    "param_names = [n for n, _ in net.named_parameters()]\n",
    "for d_name, d_hessians in zip(param_names, hessians):\n",
    "    for dd_name, dd_hessian in zip(param_names, d_hessians):\n",
    "        print(f'dz/d{dd_name}d{d_name} = \\n{dd_hessian}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(29.1231, grad_fn=<MseLossBackward0>)\n",
      "<class 'torch.nn.modules.loss.MSELoss'>\n",
      "<class 'MseLossBackward0'>\n",
      "tensor([-0.3860], grad_fn=<AddBackward0>)\n",
      "<class 'AddBackward0'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/miguel/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/miguel/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8820/2333174848.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# print(t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mhessian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, outer_jacobian_strategy)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m     res = jacobian(jac_func, inputs, create_graph=create_graph, strict=strict, vectorize=vectorize,\n\u001b[0m\u001b[1;32m    808\u001b[0m                    strategy=outer_jacobian_strategy)\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tuple_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inputs_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    576\u001b[0m                                               \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjac_func\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;31m# or else the input will be detached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jacobian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_grad_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         is_outputs_tuple, outputs = _as_tuple(outputs,\n\u001b[1;32m    576\u001b[0m                                               \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/functional.py\u001b[0m in \u001b[0;36mensure_single_output_function\u001b[0;34m(*inp)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensure_single_output_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0mis_out_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_as_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs of the user-provided function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hessian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0m_check_requires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#optimizer = optim.SGD(red.parameters(), lr=0.1, momentum=0.4)#,maximize=True)\n",
    "#optimizer.zero_grad()\n",
    "output = red_A1(torch.Tensor([1,2,3,4,5,6,7,8]))\n",
    "target = torch.Tensor([1,2,3,4,5,6,7,8])\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "print(type(criterion))\n",
    "print(type(loss.grad_fn))\n",
    "\n",
    "print(output)\n",
    "print(type(output.grad_fn))\n",
    "\n",
    "t = []\n",
    "\n",
    "for c in red_A1.parameters():\n",
    "    print(type(c.data))\n",
    "    t.append(c.data)\n",
    "\n",
    "#torch.cat(,dim = 0)\n",
    "# print(t)\n",
    "\n",
    "h = torch.autograd.functional.hessian(loss.grad_fn, torch.rand(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3211808025.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_30660/3211808025.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    h+ lambda\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(loss)\n",
    "loss.backward()\n",
    "h = torch.autograd.functional.hessian(loss)#calcula el hessiana de la funcion de perdida\n",
    "h+ lambda \n",
    "print(loss.backward())\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
