% !TEX root = ../Tesis.tex
\chapter{Implementaci贸n de algoritmos}
\label{ApB}

\section{DWT multinivel}

\begin{lstlisting}[language=Python]
import pywt

def multilevel_dwt(data,wavelet,levels,mode = 'constant'):
    """
    Calcula la transformada de ondicula (wavelet) de un conjunto de datos o serie de tiempo y devuelve sus componentes
    [IMPORTANTE] cAn, cDn hacen referencia a los coeficientes de aprocimacion y detalle al nivel n respectivamente, que surgen de la descomposicion de la serie por 
    la transformada, mientras que An y Dn son los componentes de aproximacion y detalle al nivel n, se generan al aplicar una reconstruccion parcial (upcoef) de 
    los coeficientes y son utiles para, al sumar cada uno de estos componentes, obtener la senial original.
    Args:
        data: serie o senial a descomponer
        wavelet: funcion con la cual se realizara la convolucion, y en consecuencia la descomposicion
        levels: nivel de la descomposicion
        mode:
    """
    n = len(data)
    componentes = []
    At = data

    for l in range(1,levels+1):
        # descompone la serie de tiempo en un siguiente nivel y obtiene los coeficientes de detalle y aproximacion
        (cAt, cDt) = pywt.dwt(At, wavelet, mode)
        # componentes de Aproximacion 
        At = pywt.upcoef('a', cAt, wavelet, take = n)
        # componentes de Detalle
        Dt = pywt.upcoef('d', cDt, wavelet, take = n)
        componentes[:0] = [Dt]
        if (l == levels):
            componentes[:0] = [At]
    return componentes
\end{lstlisting}

\section{Implementaci贸n NARNN}

\begin{lstlisting}[language=Python]
import torch.nn as nn
import torch.nn.functional as F

class NARNN(nn.Module):
    """
    Red Neuronal no Lineal Auto-regresiva
    
    Estructura de la red:
        Entrada: los n valores anteriores de un instante de la serie.
        Arquitectura: 3 capas densamente conectadas con 10 neuronas cada una. La primera con la funcion tangente-sigmode 
        y la segunda con logaritmo-sigmoide como funciones de activacion. Luego la capa de salida,
        con una funcion lineal como activacion comprende una sola neurona. 
        Salida: un solo valor que representa la semana consecuente a las n de entrada.
    """
    def __init__(self, t_entrada, t_salida, nombre = 'NARNN'):
        super(NARNN, self).__init__()
        self.nombre = nombre
        self.fc1 = nn.Linear(t_entrada,10)
        self.fc2 = nn.Linear(10,10)
        self.fc3 = nn.Linear(10,t_salida)

    def forward(self, x):
        """
        Paso de propagacion hacia adelante de la red
        """
        tan_sigmoid = lambda a : F.tanh(F.sigmoid(a))
        x = tan_sigmoid(self.fc1(x))
        x = F.logsigmoid(self.fc2(x))
        x = self.fc3(x)
        return x
\end{lstlisting}

\section{Implementaci贸n LSTMnn}

\begin{lstlisting}[language=Python]
from keras import Model
from keras.layers import LSTM
from keras.layers import Dropout
from keras.layers import Dense

class red_LSTM(Model):
    """
    Red Neuronal con celdas de Memoria de Corto y Largo Plazo
    
    Estructura de la red:
        Entrada: los n valores anteriores de un instante de la serie.
        Arquitectura: 4 capas con 50 celdas LSTM e intercaladas 4 capas de desactivacion (dropout)
        del 20% y finalmente una ultima capa densamente conectada que comprende una sola neurona. 
        Salida: un solo valor que representa la semana consecuente a las n de entrada.
    """
    def __init__(self,input_dim,output_dim):
        super().__init__()#red_LSTM,self
        self.LSTM1 = LSTM(units=50,return_sequences=True,input_shape=(input_dim, 1))
        self.dropout1 = Dropout(0.2)
        self.LSTM2 = LSTM(units=50,return_sequences=True)
        self.dropout2 = Dropout(0.2)
        self.LSTM3 = LSTM(units=50,return_sequences=True)
        self.dropout3 = Dropout(0.2)
        self.LSTM4 = LSTM(units=50)
        self.dropout4 = Dropout(0.2)
        self.dense = Dense(units=output_dim)

    def call(self, inputs):
        """
        Define el comportamiento del modelo cuando se llama.
        """
        x = self.LSTM1(inputs)
        x = self.dropout1(x)
        x = self.LSTM2(x)
        x = self.dropout2(x)
        x = self.LSTM3(x)
        x = self.dropout4(x)
        x = self.LSTM4(x)
        x = self.dropout4(x)
        return self.dense(x)
\end{lstlisting}

\section{Implementaci贸n GRUnn}

\begin{lstlisting}[language=Python]
from keras import Model
from keras.layers import GRU
from keras.layers import Dropout
from keras.layers import Dense

class red_GRU(Model):
    """
    Red Neuronal con Unidades Recurrentes Cerradas
    
    Estructura de la red:
        Entrada: los n valores anteriores de un instante de la serie.
        Arquitectura: 4 capas con 50 celdas LSTM e intercaladas 4 capas de desactivaciOn (dropout)
        del 20% y finalmente una Ultima capa densamente conectada que comprende una sola neurona. 
        Salida: un solo valor que representa la semana consecuente a las n de entrada.
    """
    def __init__(self,input_dim,output_dim):
        super().__init__()#red_GRU,self
        self.GRU1 = GRU(units=50,return_sequences=True,input_shape=(input_dim, 1))
        self.dropout1 = Dropout(0.2)
        self.GRU2 = GRU(units=50,return_sequences=True)
        self.dropout2 = Dropout(0.2)
        self.GRU3 = GRU(units=50,return_sequences=True)
        self.dropout3 = Dropout(0.2)
        self.GRU4 = GRU(units=50)
        self.dropout4 = Dropout(0.2)
        self.dense = Dense(units=output_dim)

    def call(self, inputs):
        """
        Define el comportamiento del modelo cuando se llama.
        """
        x = self.GRU1(inputs)
        x = self.dropout1(x)
        x = self.GRU2(x)
        x = self.dropout2(x)
        x = self.GRU3(x)
        x = self.dropout4(x)
        x = self.GRU4(x)
        x = self.dropout4(x)
        return self.dense(x)
\end{lstlisting}

\section{Algoritmo Levenberg-Marquardt}

\begin{lstlisting} [language=Python]
    def step(self):
        """
        Paso de la optimizacion del algoritmo LM
        """
        x_n = self.aux_convierte_parametros() 

        #calculamos la matriz hessiana
        h = torch.autograd.functional.hessian(self.calcula_perdida, x_n) 

        #calculamos el gradiente de la funcion
        grad_f = torch.autograd.grad(self.calcula_perdida(x_n), x_n)[0]

        # calculamos la transpuesta del gradiente
        grad_f = torch.transpose(torch.unsqueeze(grad_f, 0),0, 1) 

        # multiplica un escalar por la matriz identidad del tamanio de h y se lo sumamos a h
        h_p = h+self.lambda*torch.eye(h.size(1))
        #producto punto entre - la inversa de h_p y el gradiente de la red
        x_n1 = torch.matmul(-torch.inverse(h_p),grad_f) 
        #se le da la forma adecuada para que se pueda sumar con el vector de nuevos pesos
        x_n = x_n.reshape(h.size(1), 1)

        #se realiza la actualizacion a los parametros
        x_n = x_n + self.lr*x_n1 
        return self.asigna_parametros(torch.transpose(x_n,0,1)[0],reasignar=False)
\end{lstlisting}
